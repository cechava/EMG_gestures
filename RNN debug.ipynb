{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddb827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jun 12 2021\n",
    "@author: cechava\n",
    "\"\"\"\n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.signal\n",
    "\n",
    "#to visualize \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "#style params for figures\n",
    "sns.set(font_scale = 2)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rc(\"axes\", labelweight=\"bold\")\n",
    "\n",
    "\n",
    "#to load files\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "#ML packages\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.metrics import f1_score,make_scorer, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import Sequential, Model, load_model, Sequential, save_model\n",
    "from tensorflow. keras.layers import Dense, Activation, Dropout, Input,  TimeDistributed, GRU, Masking, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4a568e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RNN_f1(X, Y, model, average = 'weighted', mask_value = -100):\n",
    "    \"\"\"\n",
    "    Get f1 score for an RNN model using masked timepoint data\n",
    "\n",
    "    Args:\n",
    "        X: 3D numpy array with shape [samples, timepoints, features]\n",
    "        Y: 3D numpy array with shape [samples, timepoints, classes]. one-hot coding of classes\n",
    "        model: RNN model object\n",
    "        average: string argument for f1_score function. Usually 'macro' or 'weighted'\n",
    "        mask_value: value indicating which timepoints to mask out\n",
    "\n",
    "    Returns:\n",
    "        f1: f1 score\n",
    "    \"\"\"\n",
    "    # Mask out indices based on mask value\n",
    "    nonmasked_idxs = np.where(X[:,:,0].flatten()!=mask_value)[0]\n",
    "    # Get target labels for non-masked timepoints\n",
    "    y_true = np.argmax(Y,2).flatten()[nonmasked_idxs]\n",
    "    # Get model predictions for non-masked timepoints\n",
    "    preds = model.predict(X)\n",
    "    y_pred = np.argmax(preds,2).flatten()[nonmasked_idxs]\n",
    "    # Get F1 score\n",
    "    f1 = f1_score(y_true,y_pred,average = average)\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc392d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2bf11dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define where the data file are located\n",
    "data_folder = './EMG_data/01'\n",
    "\n",
    "\n",
    "#filter parameters\n",
    "lo_freq = 20\n",
    "hi_freq = 450\n",
    "\n",
    "win_size = 100 #define window size over which to compute time-domain features\n",
    "step = win_size #keeping this parameter in case I want to re-run later with some overlap\n",
    "\n",
    "#get features across segments and corresponding info\n",
    "feature_matrix, target_labels, window_tstamps, \\\n",
    "block_labels, series_labels = get_subject_data_for_classification(data_folder, lo_freq, hi_freq, \\\n",
    "                                                                  win_size, step)\n",
    "\n",
    "#exclude blocks with 'unknown' label\n",
    "in_samples = np.where(target_labels != 0)[0]\n",
    "feature_matrix_in = feature_matrix[in_samples,:]\n",
    "target_labels_in = target_labels[in_samples]\n",
    "window_tstamps_in = window_tstamps[in_samples]\n",
    "block_labels_in = block_labels[in_samples]\n",
    "\n",
    "\n",
    "feature_matrix = feature_matrix_in.T\n",
    "target_labels = target_labels_in\n",
    "window_tstamps_ = window_tstamps_in\n",
    "block_labels = block_labels_in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2191f6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Count: 1\n",
      "(18, 21, 16) (18, 21, 6)\n",
      "(6, 20, 16) (6, 20, 6)\n",
      "Train on 18 samples\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 4s 240ms/sample - loss: 1.8063 - acc: 0.0867\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 1s 28ms/sample - loss: 1.7323 - acc: 0.1579\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 23ms/sample - loss: 1.6863 - acc: 0.1548\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 23ms/sample - loss: 1.6688 - acc: 0.1455\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 24ms/sample - loss: 1.6041 - acc: 0.1920\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 25ms/sample - loss: 1.5576 - acc: 0.2477\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 24ms/sample - loss: 1.5578 - acc: 0.2105\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 27ms/sample - loss: 1.4720 - acc: 0.2910\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 27ms/sample - loss: 1.5015 - acc: 0.3003\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 1s 33ms/sample - loss: 1.4492 - acc: 0.3220\n",
      "Split Count: 2\n",
      "(18, 21, 16) (18, 21, 6)\n",
      "(6, 20, 16) (6, 20, 6)\n",
      "Train on 18 samples\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 7s 393ms/sample - loss: 1.6494 - acc: 0.1792\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 1s 50ms/sample - loss: 1.6085 - acc: 0.1981\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 1s 52ms/sample - loss: 1.5615 - acc: 0.1981\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 1s 58ms/sample - loss: 1.4817 - acc: 0.2767\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 1s 71ms/sample - loss: 1.4622 - acc: 0.2642\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 1s 44ms/sample - loss: 1.4473 - acc: 0.2736\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 1s 45ms/sample - loss: 1.4275 - acc: 0.3113\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 1s 30ms/sample - loss: 1.4087 - acc: 0.3239\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 1s 33ms/sample - loss: 1.3666 - acc: 0.3396\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 1s 35ms/sample - loss: 1.3573 - acc: 0.3585\n",
      "Split Count: 3\n",
      "(18, 20, 16) (18, 20, 6)\n",
      "(6, 21, 16) (6, 21, 6)\n",
      "Train on 18 samples\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 8s 470ms/sample - loss: 1.8199 - acc: 0.1433\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 27ms/sample - loss: 1.7666 - acc: 0.1589\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 1s 30ms/sample - loss: 1.6704 - acc: 0.2181\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 1s 35ms/sample - loss: 1.6792 - acc: 0.1589\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 1s 39ms/sample - loss: 1.6627 - acc: 0.1838\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 1s 35ms/sample - loss: 1.6237 - acc: 0.2212\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 28ms/sample - loss: 1.5688 - acc: 0.2461\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 1s 30ms/sample - loss: 1.5615 - acc: 0.2430\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 1s 45ms/sample - loss: 1.5475 - acc: 0.2492\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 1s 60ms/sample - loss: 1.5231 - acc: 0.2679\n",
      "Split Count: 4\n",
      "(18, 21, 16) (18, 21, 6)\n",
      "(6, 20, 16) (6, 20, 6)\n",
      "Train on 18 samples\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 9s 519ms/sample - loss: 1.6792 - acc: 0.1335\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 1s 28ms/sample - loss: 1.6607 - acc: 0.1056\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 25ms/sample - loss: 1.5433 - acc: 0.2019\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 25ms/sample - loss: 1.5765 - acc: 0.1739\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 26ms/sample - loss: 1.4857 - acc: 0.2236\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 27ms/sample - loss: 1.4488 - acc: 0.2857\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 24ms/sample - loss: 1.4412 - acc: 0.2764\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 26ms/sample - loss: 1.3775 - acc: 0.3540\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 1s 31ms/sample - loss: 1.3593 - acc: 0.3447\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 1s 43ms/sample - loss: 1.3286 - acc: 0.3230\n"
     ]
    }
   ],
   "source": [
    "n_splits = 4\n",
    "verbose = 1\n",
    "epochs = 10\n",
    "batch_size = 2\n",
    "permute = True\n",
    "\n",
    "#empty arrays\n",
    "train_f1_scores = np.empty((n_splits,))\n",
    "test_f1_scores = np.empty((n_splits,))\n",
    "\n",
    "#get block_ids and corresponding classes in block. there are the units over which we will do train/test split\n",
    "blocks = np.array([k for k,g in groupby(block_labels) if k!=0])\n",
    "classes = np.array([k for k,g in groupby(target_labels) if k!=0])\n",
    "\n",
    "#permute class labels, if indicated\n",
    "if permute:\n",
    "    #using indexing tricks to have this work out\n",
    "    classes_perm = np.random.permutation(classes)\n",
    "    target_labels_shuffled = np.empty((0,))\n",
    "    for i,b in enumerate(blocks):\n",
    "        idxs = np.where(block_labels==b)[0]\n",
    "        target_labels_shuffled = np.hstack((target_labels_shuffled,classes_perm[i]*np.ones((idxs.size,))))\n",
    "    target_labels = target_labels_shuffled\n",
    "    classes = classes_perm\n",
    "\n",
    "\n",
    "#stratify split to retain ratio of class labels\n",
    "skf = StratifiedKFold(n_splits=n_splits,shuffle = True)\n",
    "\n",
    "#systematically use one fold of the data as a held-out test set\n",
    "for split_count, (blocks_train_idxs, blocks_test_idxs) in enumerate(skf.split(blocks, classes)):\n",
    "    print('Split Count: %i'% (split_count+1))\n",
    "\n",
    "    #get train and test indices\n",
    "    blocks_train = blocks[blocks_train_idxs]\n",
    "    blocks_test = blocks[blocks_test_idxs]\n",
    "    train_idxs =np.where(np.isin(block_labels,blocks_train))[0]\n",
    "    test_idxs =np.where(np.isin(block_labels,blocks_test))[0]\n",
    "\n",
    "    # select training data and pad to get an array where each sample has same number of timesteps\n",
    "    X_train = feature_matrix[:,train_idxs]\n",
    "    y_train = target_labels[train_idxs]\n",
    "    #one-hot encoding of class labels\n",
    "    y_train = to_categorical(y_train-np.min(y_train))\n",
    "    #get block labels of given samples\n",
    "    win_blocks_train = block_labels[train_idxs]\n",
    "\n",
    "    #get cube\n",
    "    X_train_cube, Y_train_cube, scaler = get_data_cube(X_train, y_train,win_blocks_train, train = True, magic_value = -100)\n",
    "    print(X_train_cube.shape, Y_train_cube.shape)\n",
    "\n",
    "    # select test data and pad to get an array where each sample has same number of timesteps\n",
    "    X_test = feature_matrix[:,test_idxs]\n",
    "    y_test = target_labels[test_idxs]\n",
    "    #one-hot encoding of class labels\n",
    "    y_test = to_categorical(y_test-np.min(y_test))\n",
    "    #get block labels of given samples\n",
    "    win_blocks_test = block_labels[test_idxs]\n",
    "    #get data cube\n",
    "    X_test_cube, Y_test_cube, scaler = get_data_cube(X_test, y_test, win_blocks_test, train = False, scaler = scaler, magic_value = -100)\n",
    "    print(X_test_cube.shape, Y_test_cube.shape)\n",
    "\n",
    "    n_timesteps, n_features, n_outputs = X_train_cube.shape[1], X_train_cube.shape[2], Y_test_cube.shape[2]\n",
    "\n",
    "    #setting timestep dimension to None for variable length input\n",
    "    model = many_to_many_model((None,n_features),n_outputs,mask_value = -100)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.summary\n",
    "\n",
    "    #fit model\n",
    "    model.fit(X_train_cube, Y_train_cube, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    #evaluate model\n",
    "    train_f1_scores[split_count] = get_RNN_f1(X_train_cube, Y_train_cube, model)\n",
    "    test_f1_scores[split_count] = get_RNN_f1(X_test_cube, Y_test_cube, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "08cf60d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3002669 , 0.36841417, 0.28775529, 0.43316632])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "09e5b38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02708995, 0.17571759, 0.18739084, 0.00801887])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105e18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "196c41d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84589639, 0.63602198, 0.76269969, 0.59607104])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "62ed0f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95247002, 0.51858536, 0.57616385, 0.60577765])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733c0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
