{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"across_subject_NN_joint_training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMyz22ZnajmC4Z5oL5wmaR7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TyYD4oUuvm6h","executionInfo":{"status":"ok","timestamp":1632184995747,"user_tz":240,"elapsed":21317,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}},"outputId":"d454b16f-91e2-4b5f-e2b4-901c5df8e6ce"},"source":["#Run cell to mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70TJl7fWKDaf","executionInfo":{"status":"ok","timestamp":1632185029639,"user_tz":240,"elapsed":33902,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}},"outputId":"11be36ce-50ca-443e-aca0-4a954b6d0aa3"},"source":["# install package to have access to custom functions\n","%pip install /content/drive/MyDrive/EMG_gestures/ --use-feature=in-tree-build"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing ./drive/MyDrive/EMG_gestures\n","Building wheels for collected packages: EMG-gestures\n","  Building wheel for EMG-gestures (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for EMG-gestures: filename=EMG_gestures-0.1.0-py3-none-any.whl size=45275 sha256=9dbd7e640795a05a38f74b5a1a9266b37e7ed5dd8d908f15eb8a8e1e62f08667\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-m3jkc9no/wheels/a2/b7/61/2147fa082a9e51bef5dcc38dd3f0898fe0554d62203c0e383e\n","Successfully built EMG-gestures\n","Installing collected packages: EMG-gestures\n","Successfully installed EMG-gestures-0.1.0\n"]}]},{"cell_type":"code","metadata":{"id":"RVmD2y3kvVQm","executionInfo":{"status":"ok","timestamp":1632185032652,"user_tz":240,"elapsed":3019,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}}},"source":["#import necessary packages\n","\n","#our workhorses\n","import numpy as np\n","import pandas as pd\n","import scipy\n","\n","#to visualize\n","%matplotlib inline\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","#style params for figures\n","sns.set(font_scale = 2)\n","plt.style.use('seaborn-white')\n","plt.rc(\"axes\", labelweight=\"bold\")\n","from IPython.display import display, HTML\n","\n","#to load files\n","import os\n","import sys\n","import h5py\n","import pickle\n","from tensorflow import keras\n","\n","#append repo folder to search path\n","#import cusotm functions\n","from EMG_gestures.utils import *\n","from EMG_gestures.analysis import *\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ucHLCuHsE9f-","executionInfo":{"status":"ok","timestamp":1632185897619,"user_tz":240,"elapsed":310,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}}},"source":[" from sklearn.manifold import TSNE\n","from sklearn.model_selection import KFold\n","\n","\n","from tensorflow import keras\n","from tensorflow.keras.metrics import Precision, Recall\n","from tensorflow.keras.models import Sequential, Model, load_model, Sequential, save_model\n","from tensorflow. keras.layers import Dense, Activation, Dropout, Input,  TimeDistributed, GRU, Masking, LSTM\n","from keras.callbacks import EarlyStopping\n","\n","from tensorflow.keras.utils import to_categorical\n"," \n"," \n"," def nn_xsubject_joint_data_train_frac_subjects(feature_matrix, target_labels, sub_labels, block_labels, model_dict, exclude,\\\n","                                                    score_list, n_splits = 4,\\\n","                                                    verbose = 0, epochs = 1000, batch_size = 5, es_patience = 5,validation_split = 0.25,\\\n","                                                mv = False, permute = False):\n","    \"\"\"\n","    train and validate a vanilla neural net model using data from multiple subjects \n","    train and validate model performance by splitting subjects into a train and test set\n","    \"\"\"\n","\n","    #subjects in list. there are the units over which we will do train/test split\n","    subs = np.unique(sub_labels)\n","\n","    if permute:\n","        #permute while ignoring excluded blocks\n","        target_labels = permute_class_within_sub(target_labels, block_labels, sub_labels, exclude)\n","\n","\n","    #initialize object for k-fold cross-validation\n","    kf = KFold(n_splits=n_splits,shuffle = True)\n","    #initialize empty arrays\n","\n","    n_scores = len(score_list)\n","    train_scores_all = np.empty((n_splits,n_scores))\n","    test_scores_all = np.empty((n_splits,n_scores))\n","    train_history = dict()\n","\n","    train_info_dict = {'val_loss': np.empty((n_splits,)),\\\n","                    'train_loss': np.empty((n_splits,)),\\\n","                    'epochs_trained':np.empty((n_splits,))}\n","\n","    for split_count, (subs_train_idxs, subs_test_idxs) in enumerate(kf.split(subs)):\n","        print('Split Count: %i'% (split_count+1))\n","\n","        #get train and test indices\n","        train_subs = subs[subs_train_idxs]\n","        test_subs = subs[subs_test_idxs]\n","        train_idxs = np.where(np.isin(sub_labels,train_subs, invert = False))[0]\n","        test_idxs = np.where(np.isin(sub_labels,test_subs, invert = False))[0]\n","\n","        #get trained model\n","        train_scores, trained_model, scaler, history = get_trained_model(feature_matrix, target_labels, train_idxs, model_dict, exclude,\\\n","                                                                            score_list,\\\n","                                                                            verbose = verbose, epochs = epochs, batch_size = batch_size,\\\n","                                                                            es_patience = es_patience,\\\n","                                                                            validation_split = validation_split,\\\n","                                                                            mv = mv)\n","        #save training details to dict\n","        train_info_dict['train_loss'][split_count] = history.history['loss'][-1]\n","        train_info_dict['val_loss'][split_count] = history.history['val_loss'][-1]\n","        train_info_dict['epochs_trained'][split_count] = len(history.history['val_loss'])\n","\n","        #Evaluating on held-out subjects\n","        test_scores = evaluate_trained_nn(feature_matrix, target_labels, test_idxs, exclude, trained_model, score_list,scaler, mv = mv)\n","\n","        #put scores in array\n","        train_scores_all[split_count,:] = train_scores\n","        test_scores_all[split_count,:] = test_scores\n","\n","\n","    #put in data frame\n","    results_df = []\n","    data_dict = {'Fold':np.arange(n_splits)+1,\\\n","                    'Type':['Train' for x in range(n_splits)],\\\n","                    'Epochs':[epochs for x in range(n_splits)],\\\n","                'Batch_Size':[batch_size for x in range(n_splits)],\\\n","                'Train_Loss':train_info_dict['train_loss'],\\\n","                    'Val_Loss':train_info_dict['val_loss'],\\\n","                    'Epochs_Trained':train_info_dict['epochs_trained'],\\\n","                }\n","    for sidx in range(n_scores):\n","        data_dict['%s_score'%(score_list[sidx])] = train_scores_all[:,sidx]\n","    results_df.append(pd.DataFrame(data_dict))\n","\n","    data_dict = {'Fold':np.arange(n_splits)+1,\\\n","                    'Type':['Test' for x in range(n_splits)],\\\n","                    'Epochs':[epochs for x in range(n_splits)],\\\n","                'Batch_Size':[batch_size for x in range(n_splits)],\\\n","                'Train_Loss':train_info_dict['train_loss'],\\\n","                    'Val_Loss':train_info_dict['val_loss'],\\\n","                    'Epochs_Trained':train_info_dict['epochs_trained'],\\\n","                }\n","    for sidx in range(n_scores):\n","        data_dict['%s_score'%(score_list[sidx])] = test_scores_all[:,sidx]\n","    results_df.append(pd.DataFrame(data_dict))\n","\n","    results_df = pd.concat(results_df,axis = 0)\n","\n","    return results_df\n","\n","\n","def nn_xsubject_joint_data_train_all_subjects(feature_matrix, target_labels, sub_labels, block_labels, model_dict,exclude,\\\n","                                                     score_list, verbose = 0, epochs = 40, batch_size = 2, validation_split = 0.25,\\\n","                                              es_patience = 5, mv = False, permute = False):\n","    \"\"\"\n","    train and validate a logistic regression model using data from multiple subjects \n","    train on all subjects\n","    \"\"\"\n","\n","    #subjects in list. there are the units over which we will do train/test split\n","    subs = np.unique(sub_labels)\n","\n","    if permute:\n","        #permute while ignoring excluded blocks\n","        target_labels = permute_class_within_sub(target_labels, block_labels, sub_labels, exclude)\n","\n","\n","\n","    n_scores = len(score_list)\n","\n","\n","    train_subs = subs\n","    train_idxs = np.where(np.isin(sub_labels,train_subs, invert = False))[0]\n","    #get trained model\n","    train_scores, trained_model, scaler, history = get_trained_model(feature_matrix, target_labels, train_idxs, model_dict, exclude,\\\n","                                                                        score_list,\\\n","                                                                        verbose = verbose, epochs = epochs, batch_size = batch_size,\\\n","                                                                        es_patience = es_patience,\\\n","                                                                        validation_split = validation_split,\\\n","                                                                        mv = mv)\n","\n","\n","    #put in data frame\n","    data_dict = {'Type':'Train',\\\n","                  'Batch_Size':batch_size,\\\n","             'Train_Loss': history.history['loss'][-1],\\\n","                 'Val_Loss': history.history['loss'][-1],\\\n","                 'Epochs_Trained': len(history.history['loss'][-1]),\\\n","                 }\n","    for sidx in range(n_scores):\n","        data_dict['%s_score'%(score_list[sidx])] = train_scores[sidx]\n","    results_df = pd.DataFrame(data_dict, index = [0])\n","\n","\n","    return results_df, trained_model, scaler"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3FEEjWPMtHS","executionInfo":{"status":"ok","timestamp":1632185216407,"user_tz":240,"elapsed":175,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}}},"source":["#debug here\n","feature_matrix = feature_matrix_all.copy()\n","target_labels = target_labels_all.copy()\n","sub_labels = subject_id_all.copy()\n","block_labels =  block_labels_all.copy()\n","model_dict = {'fe_layers':1, 'fe_activation':'tanh'}\n","exclude = [0,7]\n","score_list = ['f1']\n","n_splits = 4\n","verbose = 0\n","epochs = 1000\n","batch_size = 2\n","es_patience = 5\n","mv = None\n","permute = False\n","validation_split = 0.25"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42ShMvCWMtKy","executionInfo":{"status":"ok","timestamp":1632185798009,"user_tz":240,"elapsed":298028,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}},"outputId":"9aad0d29-834b-4b39-de52-18a84714fae9"},"source":[""],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Split Count: 1\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n","Split Count: 3\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n","Split Count: 4\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n"]}]},{"cell_type":"code","metadata":{"id":"gPa9nUbCMtQN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AwDLfmBW2c8L"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BsqbYs0H6Tol","executionInfo":{"status":"ok","timestamp":1632185916928,"user_tz":240,"elapsed":171,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}}},"source":["#define hyper params for each model\n","model_dict = {0:{'fe_layers':0, 'fe_activation':''},\\\n","              1:{'fe_layers':1, 'fe_activation':'tanh'},\\\n","              2:{'fe_layers':1, 'fe_activation':'relu'},\\\n","              3:{'fe_layers':2, 'fe_activation':'tanh'},\\\n","              4:{'fe_layers':2, 'fe_activation':'relu'},\\\n","              }\n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWRLrkQFJA8m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IouUsT8S59b0","executionInfo":{"status":"ok","timestamp":1632185033178,"user_tz":240,"elapsed":10,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}}},"source":["#define where the data files are located\n","data_folder = '/content/drive/MyDrive/EMG_gestures/EMG_data/'\n","\n","nsubjects = 36\n","\n","#randomly-selected subjects to use as hold-out test data \n","test_subjects = [10, 12, 20, 14, 23, 34,  0]\n","\n","# User-defined parameters\n","lo_freq = 20 #lower bound of bandpass filter\n","hi_freq = 450 #upper bound of bandpass filter\n","\n","win_size = 100 #define window size over which to compute time-domain features\n","step = win_size #keeping this parameter in case we want to re-run later with some overlap\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDh_RNK2SztA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632185068552,"user_tz":240,"elapsed":35383,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}},"outputId":"d2995441-c8af-47bc-fabe-24e90e58639b"},"source":["#intialize empty lists\n","feature_matrix_all = np.empty((0,0))\n","target_labels_all = np.empty((0,))\n","window_tstamps_all = np.empty((0,))\n","block_labels_all  = np.empty((0,))\n","subject_id_all = np.empty((0,))\n","block_count = 0\n","\n","for subject_id in range(1,nsubjects+1):\n","    if subject_id not in test_subjects:\n","        subject_folder = os.path.join(data_folder,'%02d'%(subject_id))\n","        print('=======================')\n","        print(subject_folder)\n","\n","        # Process data and get features \n","        #get features across segments and corresponding info\n","        feature_matrix, target_labels, window_tstamps, \\\n","        block_labels, series_labels = get_subject_data_for_classification(subject_folder, lo_freq, hi_freq, \\\n","                                                                        win_size, step)\n","\n","        #prevent repeat of block labels by increasing block count\n","        block_labels = block_labels+block_count\n","        block_count = np.max([block_count, np.max(block_labels)])\n","\n","        # concatenate lists\n","        feature_matrix_all = np.vstack((feature_matrix_all,feature_matrix)) if feature_matrix_all.size else feature_matrix\n","        target_labels_all = np.hstack((target_labels_all,target_labels))\n","        window_tstamps_all = np.hstack((window_tstamps_all,window_tstamps))\n","        block_labels_all = np.hstack((block_labels_all,block_labels))\n","        subject_id_all = np.hstack((subject_id_all,np.ones((block_labels.size))*subject_id))\n","        "],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/01\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/02\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/03\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/04\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/05\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/06\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/07\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/08\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/09\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/11\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/13\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/15\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/16\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/17\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/18\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/19\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/21\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/22\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/24\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/25\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/26\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/27\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/28\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/29\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/30\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/31\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/32\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/33\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/35\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/36\n"]}]},{"cell_type":"code","metadata":{"id":"O6ylCMCgtoJe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632186549264,"user_tz":240,"elapsed":376383,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}},"outputId":"11af174d-d381-4480-de0c-9cf3bce0eeb6"},"source":["# results_folder = '/content/drive/MyDrive/EMG_gestures/results_data/xsubject_joint_data/log_reg/'\n","# figure_folder = '/content/drive/MyDrive/EMG_gestures/figures/training_history/xsubject_joint_data/log_reg'\n","\n","\n","#RNN training args - all other arguments are the same\n","verbose = 0\n","epochs = 1000\n","batch_size = 5\n","es_patience = 5\n","\n","# experiment params\n","n_splits = 4\n","nreps = 1\n","\n","#excluded labels\n","exclude = [0,7]\n","#performance metrics\n","score_list = ['f1','accuracy']\n","\n","model_id = 1\n","#for model_id in range(1,5+1):\n","np.random.seed(1)# Set seed for replicability\n","results_df = []\n","for rep in range(nreps):\n","    print('Model %d | Rep %d'%(model_id, rep+1))\n","    print('--True Data--')\n","\n","    rep_results_df = nn_xsubject_joint_data_train_frac_subjects(feature_matrix_all, target_labels_all, subject_id_all,\\\n","                                                                                    block_labels_all, model_dict[model_id], exclude,\\\n","                                                                                    score_list = score_list,\\\n","                                                                                    n_splits = n_splits,\\\n","                                                                                    verbose = 0, epochs = epochs, batch_size = batch_size,\\\n","                                                                                    es_patience = es_patience, mv = None, permute = False)\n","    #add details and concatenate dataframe\n","    rep_results_df['Shuffled'] = False\n","    rep_results_df['Rep'] =  rep+1\n","    results_df.append(rep_results_df)\n","\n","\n","\n","\n","    #repeat with shuffled data\n","    print('Model %d | Rep %d'%(model_id, rep+1))\n","    print('--Permuted Data--')\n","    rep_results_df  = nn_xsubject_joint_data_train_frac_subjects(feature_matrix_all, target_labels_all, subject_id_all,\\\n","                                                                                    block_labels_all, model_dict[model_id], exclude,\\\n","                                                                                    score_list = score_list,\\\n","                                                                                    n_splits = n_splits,\\\n","                                                                                    verbose = 0, epochs = epochs, batch_size = batch_size,\\\n","                                                                                    es_patience = es_patience, mv = None,permute = True)\n","    #add details and concatenate dataframe\n","    rep_results_df['Shuffled'] = True\n","    rep_results_df['Rep'] =  rep+1\n","    results_df.append(rep_results_df)\n","\n","#concatenate all data frames\n","results_df = pd.concat(results_df,axis = 0)\n","\n","# # #save results to file\n","# results_fn = 'model_%02d_results.h5'%(model_id)\n","# results_df.to_hdf(os.path.join(results_folder,results_fn), key='results_df', mode='w')\n"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Model 1 | Rep 1\n","--True Data--\n","Split Count: 1\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n","Split Count: 3\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n","Split Count: 4\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n","Model 1 | Rep 1\n","--Permuted Data--\n","Split Count: 1\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n","Split Count: 3\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n","Split Count: 4\n","Training Model\n","Evaluate Model on Trained Data\n","Evaluate Model\n"]}]},{"cell_type":"code","metadata":{"id":"ESnzSvc4ydR-"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":516},"id":"aJK8ovOaxK5S","executionInfo":{"status":"error","timestamp":1632186692541,"user_tz":240,"elapsed":142850,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}},"outputId":"e69ff529-0e95-44da-d181-d45e3f24deff"},"source":["results_folder = '/content/drive/MyDrive/EMG_gestures/results_data/xsubject_joint_data/NN/'\n","model_dir = '/content/drive/MyDrive/EMG_gestures/model_data/xsubject_joint_data/NN/'\n","\n","#network training args \n","verbose = 0\n","epochs = 1000\n","batch_size = 2\n","es_patience = 5\n","validation_split = 0.25\n","nreps = 10\n","\n","exclude = [0,7]\n","\n","score_list = ['f1','accuracy']#performance metrics\n","\n","model_id = 1\n","results_df = []\n","np.random.seed(1)\n","for rep in range(nreps):\n","\n","    \n","    print('Model %i || Rep %02d'%(model_id, rep+1))\n","    print('----True Data----')\n","    rep_results_df,trained_model, scaler = nn_xsubject_joint_data_train_all_subjects(feature_matrix_all, target_labels_all,\\\n","                                                                                     subject_id_all,\\\n","                                                                                     block_labels_all, model_dict[model_id],exclude,\\\n","                                                                                     score_list = score_list,\\\n","                                                                                     verbose = verbose, epochs = epochs, batch_size = batch_size,\\\n","                                                                                     es_patience = es_patience, validation_split = validation_split,\\\n","                                                                                     mv = None, permute = False)\n","    #add details and concatenate dataframe\n","    rep_results_df['Shuffled'] = False\n","    rep_results_df['Rep'] =  rep+1\n","    results_df.append(rep_results_df)\n","\n","\n","    #save trained model\n","    model_fn = os.path.join(model_folder, 'trained_model_rep_%i_all_train_data.h5'%(rep))\n","    keras.models.save_model(trained_model, model_fn, save_format= 'h5')\n","\n","    print('Model %i || Rep %02d'%(model_id, rep+1))\n","    print('----Permuted Data----')\n","    rep_results_df, b, c = nn_xsubject_joint_data_train_all_subjects(feature_matrix_all, target_labels_all,\\\n","                                                                                     subject_id_all,\\\n","                                                                                     block_labels_all, model_dict[model_id],exclude,\\\n","                                                                                     score_list = score_list,\\\n","                                                                                     verbose = verbose, epochs = epochs, batch_size = batch_size,\\\n","                                                                                     es_patience = es_patience, validation_split = validation_split,\\\n","                                                                                     mv = None, permute = False)\n","    #add details and concatenate dataframe\n","    rep_results_df['Shuffled'] = True\n","    rep_results_df['Rep'] =  rep+1\n","    results_df.append(rep_results_df)\n","\n","results_df = pd.concat(results_df, axis = 0).reset_index()\n","\n","results_fn = 'nn_joint_training_results.h5'%(model_id)\n","results_df.to_hdf(os.path.join(results_folder,results_fn), key='results_df', mode='w')\n","\n","#save scaler (outisde of rep loop b/c it will always be the same)\n","scaler_fn = 'trained_scaler_all_training_data.pkl'#\n","with open(os.path.join(model_dir,scaler_fn), \"wb\") as output_file:\n","    pickle.dump(scaler, output_file)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Model 1 || Rep 01\n","----True Data----\n","Training Model\n","Evaluate Model on Trained Data\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-f2c57d078f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model %i || Rep %02d'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----True Data----'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mrep_results_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_xsubject_joint_data_train_all_subjects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_matrix_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_labels_all\u001b[0m\u001b[0;34m,\u001b[0m                                                                                     \u001b[0msubject_id_all\u001b[0m\u001b[0;34m,\u001b[0m                                                                                     \u001b[0mblock_labels_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m                                                                                     \u001b[0mscore_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_list\u001b[0m\u001b[0;34m,\u001b[0m                                                                                     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m                                                                                     \u001b[0mes_patience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes_patience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m                                                                                     \u001b[0mmv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#add details and concatenate dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mrep_results_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Shuffled'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-bfbb4f02a4f9>\u001b[0m in \u001b[0;36mnn_xsubject_joint_data_train_all_subjects\u001b[0;34m(feature_matrix, target_labels, sub_labels, block_labels, model_dict, exclude, score_list, verbose, epochs, batch_size, validation_split, es_patience, mv, permute)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m    \u001b[0;31m#put in data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m    \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Type'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;34m'Batch_Size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'Train_Loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;34m'Val_Loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;34m'Epochs_Trained'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0msidx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m        \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%s_score'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"]}]},{"cell_type":"code","metadata":{"id":"tltKSYg2LD3c"},"source":["\n","    \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8wxpMPetY8L"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26-AW_Ulh3C-"},"source":[""],"execution_count":null,"outputs":[]}]}