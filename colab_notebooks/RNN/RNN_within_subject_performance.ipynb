{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN_within_subject_performance.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNuWIpXWc8gFi0+iBTUTSrp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Rrhi6q4ZDzt4"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ydmw6KDT22BD","executionInfo":{"status":"ok","timestamp":1632235322835,"user_tz":240,"elapsed":151,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}},"outputId":"d8fc93b3-e58f-49e6-cf8b-2fe501ec2f4c"},"source":["#Run cell to mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDk5bnX229lS","executionInfo":{"status":"ok","timestamp":1632235337052,"user_tz":240,"elapsed":13010,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}},"outputId":"79fba724-a258-45c7-83f8-36b32fc6a964"},"source":["# install package to have access to custom functions\n","%pip install /content/drive/MyDrive/EMG_gestures/ --use-feature=in-tree-build"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing ./drive/MyDrive/EMG_gestures\n","Building wheels for collected packages: EMG-gestures\n","  Building wheel for EMG-gestures (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for EMG-gestures: filename=EMG_gestures-0.1.0-py3-none-any.whl size=45311 sha256=7cbf42374105cdaa9f2d551052d942ff32bce9c2a16270e17319e64dee3a4b85\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ufllw_9m/wheels/a2/b7/61/2147fa082a9e51bef5dcc38dd3f0898fe0554d62203c0e383e\n","Successfully built EMG-gestures\n","Installing collected packages: EMG-gestures\n","  Attempting uninstall: EMG-gestures\n","    Found existing installation: EMG-gestures 0.1.0\n","    Uninstalling EMG-gestures-0.1.0:\n","      Successfully uninstalled EMG-gestures-0.1.0\n","Successfully installed EMG-gestures-0.1.0\n"]}]},{"cell_type":"code","metadata":{"id":"kolwq-Ju2-nl","executionInfo":{"status":"ok","timestamp":1632235345433,"user_tz":240,"elapsed":3390,"user":{"displayName":"Cesar Echavarria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12535184235153277849"}}},"source":["#import necessary packages\n","\n","#our workhorses\n","import numpy as np\n","import pandas as pd\n","import scipy\n","\n","#to visualize\n","%matplotlib inline\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","#style params for figures\n","sns.set(font_scale = 2)\n","plt.style.use('seaborn-white')\n","plt.rc(\"axes\", labelweight=\"bold\")\n","from IPython.display import display, HTML\n","\n","#to load files\n","import os\n","import sys\n","import h5py\n","\n","#import cusotm functions\n","from EMG_gestures.utils import *\n","from EMG_gestures.analysis import within_subject_rnn_performance\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zG2xcSRg3RRJ","outputId":"fc0befee-c881-48f3-c357-e86e712e3f1b"},"source":["#define where the data files are located\n","data_folder = '/content/drive/MyDrive/EMG_gestures/EMG_data/'\n","results_folder = '/content/drive/MyDrive/EMG_gestures/results_data/single_subject_training/RNN/'\n","\n","nsubjects = 36\n","\n","\n","# User-defined parameters\n","lo_freq = 20 #lower bound of bandpass filter\n","hi_freq = 450 #upper bound of bandpass filter\n","\n","win_size = 100 #define window size over which to compute time-domain features\n","step = win_size #keeping this parameter in case we want to re-run later with some overlap\n","\n","nreps = 10\n","exclude = [0,7]#labels to exclude\n","\n","#for RNN training\n","verbose = 0\n","epochs = 100\n","batch_size = 2\n","es_patience = 5\n","#number of permutations to use for training\n","n_shuffled_sets = 20\n","model_dict = {'fe_layers':1, 'fe_activation':'tanh'}\n","#performance metrics\n","score_list = ['f1','accuracy']\n","\n","\n","#subject_id = 1\n","for subject_id in range(1,nsubjects+1):\n","\n","    subject_folder = os.path.join(data_folder,'%02d'%(subject_id))\n","    print('=======================')\n","    print(subject_folder)\n","\n","    # Process data and get features \n","    #get features across segments and corresponding info\n","    feature_matrix_sub, target_labels_sub, window_tstamps_sub, \\\n","    block_labels_sub, series_labels_sub = get_subject_data_for_classification(subject_folder, lo_freq, hi_freq, \\\n","                                                                    win_size, step)\n","    np.random.seed(1)#for reproducibility\n","    results_df = []#initialize empty array for dataframes\n","\n","\n","    for rep in range(nreps):\n","        print('Subject %d|Rep %d'%(subject_id, rep+1))\n","        print('True Data')\n","        train_scores, test_scores,train_info_dict = within_subject_rnn_performance(feature_matrix_sub, target_labels_sub, block_labels_sub,\\\n","                                                                series_labels_sub, model_dict, exclude, score_list,\\\n","                                                                n_shuffled_sets = n_shuffled_sets,\\\n","                                                                verbose = verbose, epochs = epochs, batch_size = batch_size, es_patience = es_patience)\n","\n","        n_splits, n_scores = train_scores.shape\n","        #put testing results in dataframe\n","        data_dict = {'Fold':np.arange(n_splits)+1,\\\n","                            'Rep':[rep+1 for x in range(n_splits)],\\\n","                            'Type':['Train' for x in range(n_splits)],\\\n","                            'Shuffled':[False for x in range(n_splits)],\\\n","                            'Subject':[subject_id for x in range(n_splits)],\\\n","                     'Epochs':[epochs for x in range(n_splits)],\\\n","                'Batch_Size':[batch_size for x in range(n_splits)],\\\n","                'Train_Loss':train_info_dict['train_loss'],\\\n","                    'Val_Loss':train_info_dict['val_loss'],\\\n","                    'Epochs_Trained':train_info_dict['epochs_trained'],\\\n","                }\n","        for sidx in range(n_scores):\n","            data_dict['%s_score'%(score_list[sidx])] = train_scores[:,sidx]\n","        results_df.append(pd.DataFrame(data_dict))\n","\n","        data_dict = {'Fold':np.arange(n_splits)+1,\\\n","                            'Rep':[rep+1 for x in range(n_splits)],\\\n","                            'Type':['Test' for x in range(n_splits)],\\\n","                            'Shuffled':[False for x in range(n_splits)],\\\n","                            'Subject':[subject_id for x in range(n_splits)],\\\n","                     'Epochs':[epochs for x in range(n_splits)],\\\n","                'Batch_Size':[batch_size for x in range(n_splits)],\\\n","                'Train_Loss':train_info_dict['train_loss'],\\\n","                    'Val_Loss':train_info_dict['val_loss'],\\\n","                    'Epochs_Trained':train_info_dict['epochs_trained'],\\\n","                }\n","        for sidx in range(n_scores):\n","            data_dict['%s_score'%(score_list[sidx])] = test_scores[:,sidx]\n","        results_df.append(pd.DataFrame(data_dict))\n","        print('Subject %d|Rep %d'%(subject_id, rep+1))\n","        print('Permuted Data')\n","        target_labels_sub_perm = permute_class_within_sub(target_labels_sub, block_labels_sub, series_labels_sub, exclude)\n","        train_scores, test_scores, train_info_dict = within_subject_rnn_performance(feature_matrix_sub, target_labels_sub, block_labels_sub,\\\n","                                                                series_labels_sub, model_dict, exclude, score_list,\\\n","                                                                n_shuffled_sets = n_shuffled_sets,\\\n","                                                                verbose = verbose, epochs = epochs, batch_size = batch_size, es_patience = es_patience)\n","        n_splits, n_scores = train_scores.shape\n","        #put testing results in dataframe\n","        data_dict = {'Fold':np.arange(n_splits)+1,\\\n","                            'Rep':[rep+1 for x in range(n_splits)],\\\n","                            'Type':['Train' for x in range(n_splits)],\\\n","                            'Shuffled':[True for x in range(n_splits)],\\\n","                            'Subject':[subject_id for x in range(n_splits)],\\\n","                     'Epochs':[epochs for x in range(n_splits)],\\\n","                'Batch_Size':[batch_size for x in range(n_splits)],\\\n","                'Train_Loss':train_info_dict['train_loss'],\\\n","                    'Val_Loss':train_info_dict['val_loss'],\\\n","                    'Epochs_Trained':train_info_dict['epochs_trained'],\\\n","                }\n","        for sidx in range(n_scores):\n","            data_dict['%s_score'%(score_list[sidx])] = train_scores[:,sidx]\n","        results_df.append(pd.DataFrame(data_dict))\n","\n","        data_dict = {'Fold':np.arange(n_splits)+1,\\\n","                            'Rep':[rep+1 for x in range(n_splits)],\\\n","                            'Type':['Test' for x in range(n_splits)],\\\n","                            'Shuffled':[True for x in range(n_splits)],\\\n","                            'Subject':[subject_id for x in range(n_splits)],\\\n","                     'Epochs':[epochs for x in range(n_splits)],\\\n","                'Batch_Size':[batch_size for x in range(n_splits)],\\\n","                'Train_Loss':train_info_dict['train_loss'],\\\n","                    'Val_Loss':train_info_dict['val_loss'],\\\n","                    'Epochs_Trained':train_info_dict['epochs_trained'],\\\n","                }\n","        for sidx in range(n_scores):\n","            data_dict['%s_score'%(score_list[sidx])] = test_scores[:,sidx]\n","        results_df.append(pd.DataFrame(data_dict))\n","\n","    results_df = pd.concat(results_df, axis = 0)\n","    #save results to file\n","    results_fn = 'subject_%02d_within_subject_results.h5'%(subject_id)\n","    results_df.to_hdf(os.path.join(results_folder,results_fn), key='results_df', mode='w')\n","\n","    \n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/01\n","Subject 1|Rep 1\n","True Data\n","Split Count: 1\n","Training Model\n"]}]},{"cell_type":"code","metadata":{"id":"NjkLwq834c-V","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1630071269024,"user_tz":240,"elapsed":201,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"5404ba7a-3583-417b-b9b2-e38b487d2ab7"},"source":["results_df.groupby(['Type','Shuffled']).mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>Fold</th>\n","      <th>Rep</th>\n","      <th>Subject</th>\n","      <th>f1_score</th>\n","      <th>accuracy_score</th>\n","    </tr>\n","    <tr>\n","      <th>Type</th>\n","      <th>Shuffled</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">Test</th>\n","      <th>False</th>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>36.0</td>\n","      <td>0.919050</td>\n","      <td>0.920422</td>\n","    </tr>\n","    <tr>\n","      <th>True</th>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>36.0</td>\n","      <td>0.137796</td>\n","      <td>0.137070</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">Train</th>\n","      <th>False</th>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>36.0</td>\n","      <td>0.972319</td>\n","      <td>0.972384</td>\n","    </tr>\n","    <tr>\n","      <th>True</th>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>36.0</td>\n","      <td>0.888497</td>\n","      <td>0.891476</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                Fold  Rep  Subject  f1_score  accuracy_score\n","Type  Shuffled                                              \n","Test  False      1.5  5.5     36.0  0.919050        0.920422\n","      True       1.5  5.5     36.0  0.137796        0.137070\n","Train False      1.5  5.5     36.0  0.972319        0.972384\n","      True       1.5  5.5     36.0  0.888497        0.891476"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"t58LGCsO3RWB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S4BAkNCd0p7_"},"source":[""],"execution_count":null,"outputs":[]}]}