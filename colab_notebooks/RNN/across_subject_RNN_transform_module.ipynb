{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"across_subject_RNN_transform_module.ipynb","provenance":[],"authorship_tag":"ABX9TyMfkf9+jCh10l9TN6hnb+5Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DMIpMc4IS3g4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630099506181,"user_tz":240,"elapsed":26308,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"d6b9932b-d8a7-4993-b046-318e88bdb4c6"},"source":["#Run cell to mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vvr2T_WSTHxc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630099537198,"user_tz":240,"elapsed":31026,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"ac803851-6816-467c-f39f-7b895bf2557b"},"source":["# install package to have access to custom functions\n","%pip install /content/drive/MyDrive/EMG_gestures/ --use-feature=in-tree-build"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Processing ./drive/MyDrive/EMG_gestures\n","Building wheels for collected packages: EMG-gestures\n","  Building wheel for EMG-gestures (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for EMG-gestures: filename=EMG_gestures-0.1.0-py3-none-any.whl size=38021 sha256=7317a85bb51a8223dbb31381c631f60884cd1a128506797371cb8a7664185ffc\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-izh82i4h/wheels/a2/b7/61/2147fa082a9e51bef5dcc38dd3f0898fe0554d62203c0e383e\n","Successfully built EMG-gestures\n","Installing collected packages: EMG-gestures\n","Successfully installed EMG-gestures-0.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I6ikh9okTH4T","executionInfo":{"status":"ok","timestamp":1630099539840,"user_tz":240,"elapsed":2675,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":["#import necessary packages\n","\n","#our workhorses\n","import numpy as np\n","import pandas as pd\n","import scipy\n","\n","#to visualize\n","%matplotlib inline\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","#style params for figures\n","sns.set(font_scale = 2)\n","plt.style.use('seaborn-white')\n","plt.rc(\"axes\", labelweight=\"bold\")\n","from IPython.display import display, HTML\n","\n","#to load files\n","import os\n","import sys\n","import h5py\n","import pickle\n","from tensorflow import keras\n","\n","#append repo folder to search path\n","#import cusotm functions\n","from EMG_gestures.utils import *"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"xK9btothTPfP","executionInfo":{"status":"ok","timestamp":1630099539841,"user_tz":240,"elapsed":12,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":["#define hyper params for each model\n","model_dict = {1:{'n_grus':24, 'n_dense_pre':1, 'activation':'linear'},\\\n","              2:{'n_grus':24, 'n_dense_pre':1, 'activation':'tanh'},\\\n","              3:{'n_grus':24, 'n_dense_pre':1, 'activation':'relu'},\\\n","              4:{'n_grus':24, 'n_dense_pre':2, 'activation':'tanh'},\\\n","              5:{'n_grus':24, 'n_dense_pre':2, 'activation':'relu'},\\\n","              6:{'n_grus':24, 'n_dense_pre':3, 'activation':'tanh'},\\\n","              7:{'n_grus':24, 'n_dense_pre':3, 'activation':'relu'},\\\n","}\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"wz58gyvvTPig","executionInfo":{"status":"ok","timestamp":1630099539842,"user_tz":240,"elapsed":11,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":["#define where the data files are located\n","data_folder = '/content/drive/MyDrive/EMG_gestures/EMG_data/'\n","\n","nsubjects = 5\n","\n","#randomly-selected subjects to use as hold-out test data \n","test_subjects = [17, 23,  7,  8,  3]\n","\n","# User-defined parameters\n","lo_freq = 20 #lower bound of bandpass filter\n","hi_freq = 450 #upper bound of bandpass filter\n","\n","win_size = 100 #define window size over which to compute time-domain features\n","step = win_size #keeping this parameter in case we want to re-run later with some overlap"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlFHdNRhTPmH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630099545410,"user_tz":240,"elapsed":5578,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"1de8ecbd-9961-49b0-e9af-0a7f24de6e54"},"source":["#intialize empty lists\n","feature_matrix_all = np.empty((0,0))\n","target_labels_all = np.empty((0,))\n","window_tstamps_all = np.empty((0,))\n","block_labels_all  = np.empty((0,))\n","series_labels_all  = np.empty((0,))\n","subject_id_all = np.empty((0,))\n","block_count = 0\n","\n","for subject_id in range(1,nsubjects+1):\n","    if subject_id not in test_subjects:\n","        subject_folder = os.path.join(data_folder,'%02d'%(subject_id))\n","        print('=======================')\n","        print(subject_folder)\n","\n","        # Process data and get features \n","        #get features across segments and corresponding info\n","        feature_matrix, target_labels, window_tstamps, \\\n","        block_labels, series_labels = get_subject_data_for_classification(subject_folder, lo_freq, hi_freq, \\\n","                                                                        win_size, step)\n","\n","        #prevent repeat of block labels by increasing block count\n","        block_labels = block_labels+block_count\n","        block_count = np.max([block_count, np.max(block_labels)])\n","\n","        # concatenate lists\n","        feature_matrix_all = np.vstack((feature_matrix_all,feature_matrix)) if feature_matrix_all.size else feature_matrix\n","        target_labels_all = np.hstack((target_labels_all,target_labels))\n","        window_tstamps_all = np.hstack((window_tstamps_all,window_tstamps))\n","        block_labels_all = np.hstack((block_labels_all,block_labels))\n","        series_labels_all = np.hstack((series_labels_all,series_labels))\n","        subject_id_all = np.hstack((subject_id_all,np.ones((block_labels.size))*subject_id))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/01\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/02\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/04\n","=======================\n","/content/drive/MyDrive/EMG_gestures/EMG_data/05\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qI4GH9WATH7X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630102212941,"user_tz":240,"elapsed":1944169,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"3e1d1830-3f40-4b8c-a337-4ebce8ef21c3"},"source":["#network training args \n","verbose = 0\n","epochs = 30\n","batch_size = 5\n","\n","#validation scheme args\n","n_train_splits = 4\n","n_val_splits = 2\n","nreps = 2\n","nsets_training = 10\n","\n","#excluded labels\n","exclude = [0,7]\n","#performance metrics\n","score_list = ['f1','accuracy']\n","\n","\n","results_folder = '/content/drive/MyDrive/EMG_gestures/results_data/xsubject_transform_module/RNN/'\n","model_id = 1\n","\n","#for model_id in range(1,5+1):\n","results_model_df = []\n","np.random.seed(1)#to replicate results\n","for rep in range(nreps):\n","\n","    print('Model %i || Rep %02d'%(model_id, rep+1))\n","    print('----True Data----')\n","    rep_results_df = rnn_xsubject_transform_module_train_frac_subjects(feature_matrix_all, target_labels_all, subject_id_all, block_labels_all,\\\n","                                            series_labels_all, exclude, model_dict[model_id],score_list,\\\n","                                            n_train_splits = n_train_splits,n_val_splits = n_val_splits,\\\n","                                            nsets_training = nsets_training, verbose = verbose, epochs = epochs, batch_size = batch_size,\\\n","                                            permute = False)\n","    #add details and concatenate dataframe\n","    rep_results_df['Shuffled'] = False\n","    rep_results_df['Rep'] =  rep+1\n","    rep_results_df['Model'] = model_id\n","    results_model_df.append(rep_results_df)\n","\n","    print('Model %i || Rep %02d'%(model_id, rep+1))\n","    print('----Permuted Data----')\n","    rep_results_df = rnn_xsubject_transform_module_train_frac_subjects(feature_matrix_all, target_labels_all, subject_id_all, block_labels_all,\\\n","                                                    series_labels_all, exclude, model_dict[model_id],score_list,\\\n","                                                    n_train_splits = n_train_splits,n_val_splits = n_val_splits,\\\n","                                                    nsets_training = nsets_training, verbose = verbose, epochs = epochs, batch_size = batch_size, \\\n","                                                    permute = True)\n","    # add details and concatenate dataframe\n","    rep_results_df['Shuffled'] = True\n","    rep_results_df['Rep'] =  rep+1\n","    rep_results_df['Model'] = model_id\n","    results_model_df.append(rep_results_df)\n","\n","results_model_df = pd.concat(results_model_df,axis = 0)\n","#     #save results to file\n","#     results_fn = 'model_%02d_results.h5'%(model_id)\n","#     results_model_df.to_hdf(os.path.join(results_folder,results_fn), key='results_df', mode='w')\n","# print('***Finished!**')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Model 1 || Rep 01\n","----True Data----\n","-------Split Count: 1-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 2-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 3-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 4-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","Model 1 || Rep 01\n","----Permuted Data----\n","-------Split Count: 1-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 2-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 3-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 4-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","Model 1 || Rep 02\n","----True Data----\n","-------Split Count: 1-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 2-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 3-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 4-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","Model 1 || Rep 02\n","----Permuted Data----\n","-------Split Count: 1-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 2-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 3-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n","-------Split Count: 4-------\n","Training: Subject 01 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 02 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Training: Subject 03 out of 03\n","Training Model\n","Evaluate Model on Trained Data\n","Validation: Subject 01 out of 01\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"MLJrOUmShzoy","executionInfo":{"status":"ok","timestamp":1630105062352,"user_tz":240,"elapsed":186,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"edb533f0-18f2-4fc7-e924-0f2a779642aa"},"source":["results_model_df.groupby(['Shuffled','Type']).mean()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>Subject</th>\n","      <th>Fold</th>\n","      <th>f1_score</th>\n","      <th>accuracy_score</th>\n","      <th>Rep</th>\n","      <th>Model</th>\n","    </tr>\n","    <tr>\n","      <th>Shuffled</th>\n","      <th>Type</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">False</th>\n","      <th>Train</th>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>0.826343</td>\n","      <td>0.834566</td>\n","      <td>1.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Val_Test</th>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>0.738671</td>\n","      <td>0.736481</td>\n","      <td>1.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Val_Train</th>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>0.964272</td>\n","      <td>0.964166</td>\n","      <td>1.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">True</th>\n","      <th>Train</th>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>0.342371</td>\n","      <td>0.385475</td>\n","      <td>1.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Val_Test</th>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>0.303264</td>\n","      <td>0.323043</td>\n","      <td>1.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Val_Train</th>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>0.849205</td>\n","      <td>0.856673</td>\n","      <td>1.5</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    Subject  Fold  f1_score  accuracy_score  Rep  Model\n","Shuffled Type                                                          \n","False    Train          4.0   2.5  0.826343        0.834566  1.5    1.0\n","         Val_Test       4.0   2.5  0.738671        0.736481  1.5    1.0\n","         Val_Train      4.0   2.5  0.964272        0.964166  1.5    1.0\n","True     Train          4.0   2.5  0.342371        0.385475  1.5    1.0\n","         Val_Test       4.0   2.5  0.303264        0.323043  1.5    1.0\n","         Val_Train      4.0   2.5  0.849205        0.856673  1.5    1.0"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"59hr447fLJ7M","executionInfo":{"status":"ok","timestamp":1630095109233,"user_tz":240,"elapsed":130,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":["model_dict = model_dict[1]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kFbOIodLJ-_","executionInfo":{"status":"ok","timestamp":1630095393318,"user_tz":240,"elapsed":159,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":["feature_matrix = feature_matrix_all.copy()\n","target_labels = target_labels_all.copy()\n","sub_labels = subject_id_all.copy()\n","block_labels = block_labels_all.copy()\n","series_labels = series_labels_all.copy()\n","\n","permute = False\n","mv = None"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"P7gK-hgZLKCu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbN_Z002TH-q","executionInfo":{"status":"ok","timestamp":1630099566928,"user_tz":240,"elapsed":385,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"WyQkoVgBWebA","executionInfo":{"status":"ok","timestamp":1630099368467,"user_tz":240,"elapsed":174,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"95187bdc-9b2f-4800-bf19-48bc7f9a601c"},"source":["results_df.groupby('Type').mean()"],"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Subject</th>\n","      <th>Fold</th>\n","      <th>f1_score</th>\n","      <th>accuracy_score</th>\n","    </tr>\n","    <tr>\n","      <th>Type</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Train</th>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>0.815120</td>\n","      <td>0.832551</td>\n","    </tr>\n","    <tr>\n","      <th>Val_Test</th>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>0.722993</td>\n","      <td>0.729840</td>\n","    </tr>\n","    <tr>\n","      <th>Val_Train</th>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>0.968468</td>\n","      <td>0.968482</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Subject  Fold  f1_score  accuracy_score\n","Type                                              \n","Train          4.0   2.5  0.815120        0.832551\n","Val_Test       4.0   2.5  0.722993        0.729840\n","Val_Train      4.0   2.5  0.968468        0.968482"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"HswKr8j6WeeP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4gsNVA-WdKl","executionInfo":{"status":"ok","timestamp":1630097840284,"user_tz":240,"elapsed":6833,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"8e5102d2-f5bd-4078-f57e-553e4273250f"},"source":[""],"execution_count":76,"outputs":[{"output_type":"stream","text":["Validation: Subject 01 out of 01\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dhqGC7w3TIDL","colab":{"base_uri":"https://localhost:8080/","height":263},"executionInfo":{"status":"ok","timestamp":1630097853937,"user_tz":240,"elapsed":191,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"47f2847b-1c2d-4c79-a899-30097dbddcf5"},"source":["results_df.groupby"],"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Subject</th>\n","      <th>Fold</th>\n","      <th>Type</th>\n","      <th>f1_score</th>\n","      <th>accuracy_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.0</td>\n","      <td>4</td>\n","      <td>Train</td>\n","      <td>0.713112</td>\n","      <td>0.736952</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.0</td>\n","      <td>4</td>\n","      <td>Train</td>\n","      <td>0.894133</td>\n","      <td>0.894775</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.0</td>\n","      <td>4</td>\n","      <td>Train</td>\n","      <td>0.913993</td>\n","      <td>0.914367</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2.0</td>\n","      <td>4</td>\n","      <td>Val_Train</td>\n","      <td>0.976725</td>\n","      <td>0.976744</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.0</td>\n","      <td>4</td>\n","      <td>Val_Train</td>\n","      <td>0.970112</td>\n","      <td>0.957746</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2.0</td>\n","      <td>4</td>\n","      <td>Val_Test</td>\n","      <td>0.726306</td>\n","      <td>0.746479</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.0</td>\n","      <td>4</td>\n","      <td>Val_Test</td>\n","      <td>0.965126</td>\n","      <td>0.965116</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Subject  Fold       Type  f1_score  accuracy_score\n","0      3.0     4      Train  0.713112        0.736952\n","1      6.0     4      Train  0.894133        0.894775\n","2      5.0     4      Train  0.913993        0.914367\n","0      2.0     4  Val_Train  0.976725        0.976744\n","1      2.0     4  Val_Train  0.970112        0.957746\n","0      2.0     4   Val_Test  0.726306        0.746479\n","1      2.0     4   Val_Test  0.965126        0.965116"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"_1WpX7CVTIdq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lHJlqNPfTIhz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBZtjnl5TIno"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLKVNU6oTIqv","executionInfo":{"status":"ok","timestamp":1630099572457,"user_tz":240,"elapsed":172,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":["#ML packages\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score,make_scorer, log_loss\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from sklearn.manifold import TSNE\n","from sklearn.model_selection import KFold\n","\n","\n","from tensorflow import keras\n","from tensorflow.keras.metrics import Precision, Recall\n","from tensorflow.keras.models import Sequential, Model, load_model, Sequential, save_model\n","from tensorflow. keras.layers import Dense, Activation, Dropout, Input,  TimeDistributed, GRU, Masking, LSTM\n","\n","from tensorflow.keras.utils import to_categorical"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"dZMlt8InPkss"},"source":["def rnn_xsubject_transform_module_train_all_subjects(feature_matrix, target_labels, sub_labels, block_labels,\\\n","                                                         train_idxs, test_idxs, exclude, model_dict, score_list,\\\n","                                                         figure_folder = '', model_folder = '', nsets_training = 10,\\\n","                                                         verbose = 0, epochs = 40, batch_size = 2, mv = None, permute = False):\n","    \"\"\"\n","    train and validate an RNN model with a transform module for domain adaptation. \n","    validate model performance by holding out indicated samples for each subject\n","\n","    \"\"\"\n","\n","    #default values\n","    if 'n_dense_post' not in model_dict.keys():\n","        model_dict['n_dense_post'] = 0\n","    if 'n_grus' not in model_dict.keys():\n","        model_dict['n_grus'] = 24\n","\n","    results_df = []\n","    subs = np.unique(sub_labels)\n","\n","    if permute:\n","        #permute while ignoring excluded blocks\n","        target_labels = permute_class_within_sub(target_labels, block_labels, sub_labels, exclude)\n","\n","    in_samples = np.where(np.isin(target_labels,exclude, invert = True))[0]\n","    #get training data cubes\n","\n","    X_train_cube, Y_train_cube, scaler = prepare_data_for_RNN(feature_matrix, target_labels, train_idxs, exclude, train = True,\\\n","                                                                block_labels = block_labels, nsets = nsets_training)\n","    sub_labels_train = sub_labels[np.intersect1d(train_idxs,in_samples)]\n","\n","    #testfor equal number of samples\n","    assert X_train_cube.shape[0] == Y_train_cube.shape[0]\n","    #testfor equal number of timepoints\n","    assert X_train_cube.shape[1] == Y_train_cube.shape[1]\n","    n_features, n_outputs = X_train_cube.shape[2], Y_train_cube.shape[2]\n","\n","    if test_idxs.size>0:\n","        #get testing data cubes\n","        X_test_cube, Y_test_cube, scaler = prepare_data_for_RNN(feature_matrix, target_labels, test_idxs, exclude, train = False, scaler = scaler)\n","        sub_labels_test = sub_labels[np.intersect1d(test_idxs,in_samples)]\n","\n","    # permute order in which subjects' data is used for training\n","    subs_perm = np.random.permutation(subs)\n","\n","    #initialize empty list\n","    n_scores = len(score_list)\n","    train_scores = np.empty((subs.size,n_scores))\n","    test_scores = np.empty((subs.size,n_scores))\n","\n","    # --- Training Stage ---\n","    # Define model architecture\n","\n","    #setting timestep dimension to None \n","    model = get_rnn_model((None,n_features,),n_outputs,n_dense_pre=model_dict['n_dense_pre'], n_dense_post=model_dict['n_dense_post'],\\\n","                            n_grus = model_dict['n_grus'], activation=model_dict['activation'])\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    #model.summary\n","    # # Get transform module template\n","\n","    transform_module_template = get_transform_module(model, Input(shape = (None,n_features)),3 + model_dict['n_dense_post'])\n","\n","\n","    # iterate thorugh subjects' data\n","    for sub_idx, train_sub in enumerate(subs_perm):\n","        print('Training: Subject %02d out of %02d'%(sub_idx+1, subs_perm.size))\n","        # get subject-specific samples\n","        train_sub_idxs = np.where(sub_labels_train == train_sub)[0]\n","\n","        X_cube_sub = X_train_cube[:,train_sub_idxs,:]\n","        Y_cube_sub = Y_train_cube[:,train_sub_idxs,]\n","        # initialize weights of the transform module\n","        model = tm_template_weights_to_model(transform_module_template, model)\n","\n","        print('Training Model')\n","        # fit network\n","        history = model.fit(X_cube_sub, Y_cube_sub, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","        if figure_folder:\n","            #plot training loss\n","            fig_title = 'Subject %02d'%(train_sub)\n","            fig_fn = os.path.join(figure_folder,'rnn_model_subject_%02d_all_train_data_permuted_%s_loss.png'%(train_sub,str(permute)))\n","            plot_train_loss(history, fig_title, fig_fn)\n","\n","        #copy weights to a transfer module template, save if wanted\n","        trained_transfer_module = model_weights_to_tm_template(transform_module_template, model)\n","        if model_folder:\n","            #save trained transfer module to file\n","            model_fn = os.path.join(model_folder, 'transform_module_subject_%02d_all_train_data_permuted_%s.h5'%(train_sub, str(permute)))\n","            keras.models.save_model(trained_transfer_module, model_fn, save_format= 'h5')\n","        # # evaluate trained network\n","        print('Evaluate Model on Trained Data')\n","\n","        if mv:\n","            # get f1 score after applying majority voting scheme to model predictions\n","            train_scores[sub_idx,:]  = apply_mv_and_get_scores(feature_matrix, target_labels,\\\n","                                                                np.intersect1d(np.where(sub_labels==train_sub)[0],train_idxs), exclude,\\\n","                                                                scaler, model, mv, score_list, rnn = True)\n","            if test_idxs.size>0:\n","                test_scores[sub_idx,:] = apply_mv_and_get_scores(feature_matrix, target_labels, \\\n","                                                                    np.intersect1d(np.where(sub_labels==train_sub)[0],test_idxs), exclude,\\\n","                                                                    scaler, model, mv, score_list, rnn = True)\n","        else:\n","            #get score for training data\n","            train_scores[sub_idx,:]  = get_scores(X_cube_sub, Y_cube_sub, model, score_list, rnn = True)\n","            if test_idxs.size>0:\n","                #get score for test data\n","                test_sub_idxs = np.where(sub_labels_test == train_sub)[0]\n","                test_scores[sub_idx,:]  = get_scores(X_test_cube[:,test_sub_idxs,:], Y_test_cube[:,test_sub_idxs,:], model, score_list, rnn = True)\n","\n","    #put results in dataframe\n","    data_dict = {'Subject':subs_perm,\\\n","                 'Type':['Train' for x in range(subs_perm.size)]}\n","    for sidx in range(n_scores):\n","        data_dict['%s_score'%(score_list[sidx])] = train_scores[:,sidx]\n","    results_df.append(pd.DataFrame(data_dict))\n","\n","    if test_idxs.size>0:\n","            data_dict = {'Subject':subs_perm,\\\n","                         'Type':['Train_val' for x in range(subs_perm.size)]}\n","            for sidx in range(n_scores):\n","                data_dict['%s_score'%(score_list[sidx])] = test_scores[:,sidx]\n","            results_df.append(pd.DataFrame(data_dict))\n","        \n","    results_df = pd.concat(results_df,axis = 0)\n","\n","    if model_folder:\n","        #save complete model to file\n","        model_fn = os.path.join(model_folder, 'trained_model_all_train_data_permuted_%s.h5'%(str(permute)))\n","        keras.models.save_model(model, model_fn, save_format= 'h5')\n","    return results_df, scaler"],"execution_count":null,"outputs":[]}]}