{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"colab":{"name":"RNN debug.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"8a622c9c"},"source":["\n","#!/usr/bin/env python2\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat Jun 12 2021\n","@author: cechava\n","\"\"\"\n","from itertools import groupby\n","import numpy as np\n","import pandas as pd\n","import scipy\n","import scipy.signal\n","\n","#to visualize \n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","#style params for figures\n","sns.set(font_scale = 2)\n","plt.style.use('seaborn-white')\n","plt.rc(\"axes\", labelweight=\"bold\")\n","\n","\n","#to load files\n","import os\n","import h5py\n","\n","#ML packages\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.metrics import f1_score,make_scorer, log_loss\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from sklearn.manifold import TSNE\n","\n","\n","from tensorflow import keras\n","from tensorflow.keras.metrics import Precision, Recall\n","from tensorflow.keras.models import Sequential, Model, load_model, Sequential, save_model\n","from tensorflow. keras.layers import Dense, Activation, Dropout, Input,  TimeDistributed, GRU, Masking, LSTM\n","from tensorflow.keras.utils import to_categorical\n"],"id":"8a622c9c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0D-4n81T3MgH"},"source":["\n","#!/usr/bin/env python2\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat Jun 12 2021\n","@author: cechava\n","\"\"\"\n","from itertools import groupby\n","import numpy as np\n","import pandas as pd\n","import scipy\n","import scipy.signal\n","\n","#to visualize \n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","#style params for figures\n","sns.set(font_scale = 2)\n","plt.style.use('seaborn-white')\n","plt.rc(\"axes\", labelweight=\"bold\")\n","\n","\n","#to load files\n","import os\n","import h5py\n","\n","#ML packages\n","from sklearn.linear_model import  LogisticRegression\n","from sklearn.metrics import f1_score,make_scorer, log_loss\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from sklearn.manifold import TSNE\n","\n","\n","from tensorflow import keras\n","from tensorflow.keras.metrics import Precision, Recall\n","from tensorflow.keras.models import Sequential, Model, load_model, Sequential, save_model\n","from tensorflow. keras.layers import Dense, Activation, Dropout, Input,  TimeDistributed, GRU, Masking, LSTM\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","# ~~~~~~~ DATA WRANGLING FUNCTIONS ~~~~~~~\n","def get_gesture_times(data_df):\n","    \"\"\"\n","    Get start times, end times, and event labels of each block of time with a given label\n","    \n","    Args:\n","        data_df: dataframe with columns for 'class' and 'time' \n","            \n","    Returns:\n","        start times, end times, and event labels: 1-d numpy arrays \n","        \n","    \"\"\"\n","\n","    #get start indices of each condition \n","    \n","    #taking advantage that every gesture is preceded by a period with 'undefined' class label (value 0)\n","    start_idxs = np.hstack([0,np.where(np.abs(np.diff(data_df['class']))>0)[0]+1])\n","    \n","    #end indices correspond to just before the start of the next condition\n","    end_idxs = np.hstack((start_idxs[1:]-1,data_df.time.size-1))\n","    \n","\n","    #create arrays with start times and class labels for each hand gesture\n","    start_times = data_df.time[start_idxs].values\n","    end_times = data_df.time[end_idxs].values\n","    event_labels = data_df['class'][start_idxs].values\n","    \n","    return start_times, end_times, event_labels\n","\n","def get_steady_samp_rate_data(data_df):\n","    \n","    \"\"\"\n","    Resample time series data for a steady sampling rate by performing linear interpolation\n","    between missing samples\n","    \n","    Args:\n","        data_df: dataframe with a 'time' column and timeseries of interest on other columns\n","            \n","    Returns:\n","        data_df: dataframe with interpolated \n","    \"\"\"\n","    \n","    #retrieve time value\n","    time = data_df.time.values\n","    #define time points on which we want to interpolate (i.e., a sample every ms)\n","    time_new = np.arange(time[0],time[-1]+1,1)\n","    \n","    #create separate dataframe with new time points and set these as index\n","    steady_time_df = pd.DataFrame({'time_constant':time_new})\n","    steady_time_df = steady_time_df.set_index('time_constant')\n","    \n","    #set time coulumn of original dataframe to index\n","    data_df = data_df.set_index('time')\n","\n","    #perform a right join with the steady time dataframe then linearly interpolate missing values\n","    data_df = data_df.join(steady_time_df, how = 'right')\n","    data_df = data_df.interpolate().reset_index().rename(columns={'time_constant':'time'})\n","    \n","    return data_df\n","\n","def butter_bandpass_filter(input_signal, lowcut, highcut, fs, order=4, axis = -1):\n","    #lowcut = lower bound of desired freq band\n","    #hicut = upperbound bound of desired freq band\n","    #fs = sampling rate\n","    #order = order of filter\n","    #axis = axis of data matrix on which to apply filters\n","\n","    low = float(lowcut)# / nyq\n","    high = float(highcut)# / nyq\n","    b, a = scipy.signal.butter(order, [low, high],fs = fs, btype='band')\n","    \n","    output_signal = scipy.signal.filtfilt(b, a, input_signal,axis = axis)\n","    \n","    return output_signal\n","    \n","def get_window_features(signal):\n","    #signal: EMG signal matrix with dims time x channels\n","    #return: Mean Absolute Value and Wavelength features for each channel\n","\n","    MAV = np.mean(np.absolute(signal),axis = 0) #Mean Absolute Value\n","\n","    WL = np.sum(np.absolute(np.diff(signal,axis = 0)),axis = 0) #Wavelength\n","    \n","    return MAV, WL\n","\n","def window_and_get_features(parsed_df,win_size,step):\n","    \"\"\"\n","    Compute features (Mean Absolute Value and Wavelength) for time series segments with given length \n","    and amount of overlap\n","    \n","    Args:\n","        parsed_df: dataframe containing timeseries data for each block. expects a 'Block' column\n","        and the channel data in the first columns of dataframe\n","        \n","            \n","    Returns:\n","        feat_matrix: 2D numpy array of size [number of segments, number of channels * number of features]\n","        target_labels: 1D numpy array with class label for each segment\n","        window_tstamps: 1D numpy array with timestamp for each timeseries segment used to compute features\n","        block_labels: 1D numpy array indicating block provenance of each segment (useful for RNN data prep)\n","    \"\"\"\n","    #set empty lists\n","    feat_matrix = []\n","    target_labels = []\n","    window_tstamps = []\n","    block_labels = []\n","\n","    #get number of channels\n","    nchannels = np.sum(['Channel' in col_name for col_name in parsed_df.columns])\n","\n","    for block in parsed_df.Block.unique():\n","        #get relevant subset of dataframe\n","        block_df = parsed_df.loc[parsed_df.Block == block]\n","\n","        #extract relevant info\n","        block_data_matrix = block_df.iloc[:,0:8].values\n","        block_class = block_df['Class'][0]\n","        tstamps = block_df['Time'].values\n","\n","        #compute desired features over segments of the data\n","        for win_start_idx in range(0, block_df.shape[0], step):\n","            win_end_idx = win_start_idx + win_size\n","            if win_end_idx< block_df.shape[0]: #exclude window if not enough timepoints before end of block\n","\n","                #compute features within this segment\n","                MAV, WL = get_window_features(block_data_matrix[win_start_idx:win_end_idx,:])\n","\n","                #append info to matrices\n","                feat_matrix.append(np.hstack((MAV,WL)))\n","                target_labels.append(block_class)\n","                window_tstamps.append(np.mean([tstamps[win_start_idx],tstamps[win_end_idx]]))\n","                block_labels.append(block)\n","                \n","    return np.array(feat_matrix), np.array(target_labels), np.array(window_tstamps), np.array(block_labels)\n","\n","\n","\n","\n","\n","def parse_data_blocks(start_times, end_times, event_labels, data_matrix, tstamps, pre_tpts = 0, exclude_class=[]):\n","    \"\"\"\n","    Function to parse timeseries into blocks corresponding to each event. \n","    Returns a dataframe - seemed most convenient given variable length of event blocks\n","    \n","    Args:\n","        start_times: 1D numpy array with start times of event blocks\n","        end_times: 1D numpy array with start times of event blocks\n","        event_labels: 1D numpy array with start times of event blocks\n","        data_matrix: 2D numpy array with of dimension [ntimepoints, nchannels] containing signal values\n","        pre_tpts: how many timepoints before event onset to include(int)\n","        exclude_class: list with class labels to exclude in parsing blocks\n","            \n","    Returns:\n","        parsed_df: dataframe with parsed data\n","        block_length: 1D numpy array with timepoints in each event block\n","    \"\"\"\n","    \n","    parsed_df = []\n","\n","    #note characterisitcs of matrix\n","    ntpts, nchannels = data_matrix.shape\n","\n","    parsed_df = []\n","    block_lengths = []\n","    for block,c in enumerate(event_labels):\n","        if c not in exclude_class:\n","            # get relevant indives to get dara from matrix\n","            start_idx = np.where(tstamps>start_times[block])[0][0]\n","            end_idx = np.where(tstamps>=end_times[block])[0][0]\n","\n","            # append block length to list\n","            block_lengths.append(end_idx-start_idx)\n","\n","            # get timestamps relative to event onset\n","            t_ase = np.arange(-pre_tpts,end_idx-start_idx)\n","\n","            # put data matrix values into a dataframe\n","            block_df = pd.DataFrame(data_matrix[start_idx-pre_tpts:end_idx,:], columns = ['Channel_%i'%(c+1) for c in range(nchannels)])\n","\n","            # add relevant value columns\n","            block_df['Block'] = block\n","            block_df['Class'] = c \n","            block_df['Time_ASE'] = t_ase\n","            block_df['Time'] = np.arange(tstamps[start_idx]-pre_tpts,tstamps[end_idx])\n","\n","            #append to collecting datafram\n","            parsed_df.append(block_df)\n","\n","    #merge all dataframes together\n","    parsed_df = pd.concat(parsed_df,axis = 0)\n","    \n","    return parsed_df, block_lengths\n","\n","def get_file_data_for_classification(data_fn, lo_freq, hi_freq, win_size, step, remove_file_mean = True):\n","    \"\"\"\n","    Get the relevant info for classification from indicated file\n","    \n","    Args:\n","        data_fn: filename\n","        lo_freq: lower bound of bandpass filter\n","        hi_freq: higher bound of bandpass filter\n","        win_size: length of segment over which to compute features\n","        step: amount of overlap between neighboring segments\n","        remove_file_mean: Boolean indicating removal of mean value for each channel in the series\n","            \n","    Returns:\n","        feature_matrix: 2D numpy array of size [number of segments, number of channels * number of features]\n","        target_labels: 1D numpy array with class label for each segment\n","        window_tstamps: 1D numpy array with timestamp for each timeseries segment used to compute features\n","        block_labels: 1D numpy array indicating block provenance of each segment (useful for RNN data prep)\n","    \"\"\"\n","    \n","    # load file\n","    data_df = pd.DataFrame(pd.read_csv(data_fn, sep='\\t'))\n","    \n","    # retrieve start times, end times, and labels for each condition block (will make it easier to parse traces later)\n","    start_times, end_times, event_labels  = get_gesture_times(data_df)\n","\n","    # interpolate data to steady frame rate\n","    data_df = get_steady_samp_rate_data(data_df)\n","    samp_period = np.diff(data_df.time)[0]/1000.0\n","    samp_rate = 1/samp_period\n","    \n","    # Unpack values into numpy arrays\n","    data_matrix = data_df.iloc[:,1:-1].values\n","    class_labels = data_df.iloc[:,-1].values\n","    tstamps = data_df.time.values\n","    \n","    #remove series offset for each channel, if indicated\n","    if remove_file_mean:\n","        data_matrix = data_matrix- np.mean(data_matrix,0)\n","\n","    # filter data\n","    filt_data_matrix = butter_bandpass_filter(data_matrix, lo_freq, hi_freq, samp_rate,axis = 0)\n","\n","    # parse timeseries into block corresponding to different blocks of time\n","    parsed_df, block_lengths = parse_data_blocks(start_times,end_times,event_labels,filt_data_matrix, tstamps,\n","                                                 pre_tpts = 0, exclude_class=[])\n","\n","    # compute desired features over individual time segments\n","    feature_matrix, target_labels, window_tstamps, block_labels = window_and_get_features(parsed_df,win_size,step)\n","    \n","    return feature_matrix, target_labels, window_tstamps, block_labels\n","\n","\n","def get_subject_data_for_classification(data_folder, lo_freq, hi_freq, win_size, step):\n","    \"\"\"\n","    Get the relevant info for classification from indicated file\n","    \n","    Args:\n","        data_fn: filename\n","        lo_freq: lower bound of bandpass filter\n","        hi_freq: higher bound of bandpass filter\n","        win_size: length of segment over which to compute features\n","        step: amount of overlap between neighboring segments\n","            \n","    Returns:\n","        feature_matrix: 2D numpy array of size [number of segments, number of channels * number of features]\n","        target_labels: 1D numpy array with class label for each segment\n","        window_tstamps: 1D numpy array with timestamp for each timeseries segment used to compute features\n","        block_labels: 1D numpy array indicating block provenance of each segment (useful for RNN data prep)\n","        series_labels: 1D numpy array indicating block provenance of each segment (useful for visualization)\n","    \"\"\"\n","    \n","    #find files in subject folder\n","    file_list = [f for f in os.listdir(data_folder) if os.path.isfile(os.path.join(data_folder, f))]\n","\n","\n","    #initialize empty matrices\n","    feature_matrix = np.empty((0,0))\n","    target_labels = np.empty((0,))\n","    window_tstamps = np.empty((0,))\n","    block_labels = np.empty((0,))\n","    series_labels = np.empty((0,))\n","    max_block_id = 0\n","\n","    for series_id,file in enumerate(file_list):\n","\n","        #get relevant info from each file\n","        feature_matrix_sub, target_labels_sub,\\\n","        window_tstamps_sub, block_labels_sub = get_file_data_for_classification(os.path.join(data_folder,file),\\\n","                                                                                 lo_freq, hi_freq, win_size, step)\n","        nsamples,nfeats = feature_matrix_sub.shape\n","\n","        #offset block labels\n","        block_labels_sub = block_labels_sub+max_block_id\n","        max_block_id = np.max(block_labels_sub)#update\n","\n","        series_labels_sub = np.ones((nsamples,))*series_id#make array with series ID of samples\n","\n","        #append file samples\n","        feature_matrix = np.vstack((feature_matrix,feature_matrix_sub)) if feature_matrix.size else feature_matrix_sub\n","        target_labels = np.hstack((target_labels,target_labels_sub))\n","        window_tstamps = np.hstack((window_tstamps,window_tstamps_sub))\n","        block_labels = np.hstack((block_labels,block_labels_sub))\n","        series_labels = np.hstack((series_labels,series_labels_sub))\n","        \n","    return feature_matrix, target_labels, window_tstamps, block_labels, series_labels\n","\n","\n","def get_data_cube(X, window_blocks, train = True, scaler = None, magic_value = -100):\n","    \"\"\"\n","    Create data cube for use with Keras RNN. Standardize data then pad and reshape data to have\n","    [samples, timesteps, features] dimensions with an equal number of timesteps for each slice\n","    I use a Masking layer in the RNN architecture to allow for sequences of different length\n","    \n","    Args:\n","        X: 2D nuumpy array with data, dimensions [features, samples]\n","        window_blocks: 1D numpy array indicating block of provenance for input segment values\n","        train: Boolean indicating whether the input data is training data\n","        scaler: StandardScaler to transform data\n","        magic_value: integer indicating value with which to pad samples\n","            \n","    Returns:\n","        X_cube: 3D numpy array of size [samples, timesteps, features]\n","        scaler: 1D numpy array with class label for each segment\n","    \"\"\"\n","    #standardize across each feature dimension\n","    if train:\n","        scaler = StandardScaler()\n","        scaler = scaler.fit(X.T)\n","        X = scaler.transform(X.T).T\n","    else:\n","        #for testing data, we want to use same transform as was fit to training data\n","        X = scaler.transform(X.T).T\n","        \n","    # common number of time steps\n","    common_timesteps = np.max(np.bincount(window_blocks.astype('int')))\n","    \n","    # get each block, pad, and stack to form a data cube\n","    X_cube = []\n","    for b_count, b_idx in enumerate(np.unique(window_blocks)):\n","       #slice\n","        X_slice = X[:,np.where(window_blocks==b_idx)[0]]\n","        #pad\n","        pad_size = common_timesteps-X_slice.shape[1]\n","        X_slice_pad = np.pad(X_slice,pad_width=((0,0),(0,pad_size)), mode='constant', constant_values= magic_value)\n","        #stack\n","        if b_count == 0:\n","            X_cube  = X_slice_pad\n","        else:\n","            X_cube = np.dstack((X_cube,X_slice_pad))\n","\n","    # swap dimension to get [samples, timesteps, features]\n","    X_cube = np.swapaxes(X_cube,0,2)\n","    \n","    return X_cube, scaler\n","\n","#~~~~~ VISUALIZATION FUNCTIONS ~~~~~~\n","\n","def plot_sensor_values(data_fn, x_limits = []):\n","    \"\"\"\n","    Plot signal timecourse for all channels using data in indicated file\n","    \n","    Args:\n","        data_fn: filename\n","        x_limits: minimum and maximum limits for x-axis (useful to looking at specific sections)\n"," \n","    Returns:\n","        fig: figure handle\n","    \"\"\"\n","\n","    # load file\n","    data_df = pd.DataFrame(pd.read_csv(data_fn, sep='\\t'))\n","\n","    # retrieve start times, end times, and labels for each condition block (will make it easier to parse traces later)\n","    start_times, end_times, event_labels  = get_gesture_times(data_df)\n","\n","    # interpolate data to steady frame rate\n","    data_df = get_steady_samp_rate_data(data_df)\n","\n","    # Unpack values into numpy arrays\n","    data_matrix = data_df.iloc[:,1:-1].values\n","    class_labels = data_df.iloc[:,-1].values\n","    tstamps = data_df.time.values\n","\n","    # define color palette\n","    palette = sns.color_palette('deep',8)\n","\n","    # define class legend\n","    labels = []\n","    custom_lines = []\n","\n","    classes = np.unique(event_labels)[1:]#exclude 'unmarked' label\n","    for c in classes.astype('int'):\n","        labels.append('Class %i'%(c))\n","        custom_lines.append(Line2D([0], [0], color=palette[c-1], lw=4))\n","\n","    #make figure\n","    nrows = 8\n","    ncols = 1\n","\n","    fig,ax = plt.subplots(nrows,ncols,figsize=(15,30),sharex = True)\n","\n","    #plot each channel\n","    for ch in range(8):\n","        ax[ch//ncols].plot(tstamps,data_matrix[:,ch]);\n","        ax[ch//ncols].axhline(y = 0, xmin = 0, xmax = 1, color = 'k', linestyle = '--', alpha = 0.5)\n","        #label subplot\n","        ax[ch//ncols].set_title('Channel %i'%(ch+1))\n","\n","        #mark events\n","        ymin,ymax = ax[ch//ncols].get_ylim()\n","        for idx,c in enumerate(event_labels):\n","            if c>0:#eclude 'unmarked label'\n","                ax[ch//ncols].hlines(y = ymax + .001, xmin = start_times[idx], xmax = end_times[idx], color = palette[int(c-1)],linewidth = 10)\n","\n","        if len(x_limits):\n","            ax[ch//ncols].set_xlim(x_limits)\n","\n","\n","    #set legend with events\n","    ax[0].legend(custom_lines, labels,bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n","\n","\n","    #label axes\n","    ax[0].set_ylabel('Sensor Voltage')\n","    ax[ch].set_xlabel('Time (ms)')\n","\n","    #despine\n","    sns.despine(fig= plt.gcf(), left = False, right = True, top = True, bottom = True)\n","\n","\n","    fig.tight_layout() \n","\n","    return fig\n","\n","def plot_signal_pspec(data_fn):\n","    \"\"\"\n","    Plot power spectrum of signal for each channel using data in datafile indicated\n","    \n","    Args:\n","        data_fn: filename\n"," \n","    Returns:\n","        fig: figure handle\n","    \"\"\"\n","\n","    # load file\n","    data_df = pd.DataFrame(pd.read_csv(data_fn, sep='\\t'))\n","\n","    # retrieve start times, end times, and labels for each condition block (will make it easier to parse traces later)\n","    start_times, end_times, event_labels  = get_gesture_times(data_df)\n","\n","    # interpolate data to steady sampling rate\n","    data_df = get_steady_samp_rate_data(data_df)\n","    samp_period = np.diff(data_df.time)[0]/1000.0\n","    samp_rate = 1/samp_period\n","\n","    # Unpack values into numpy arrays\n","    data_matrix = data_df.iloc[:,1:-1].values\n","    class_labels = data_df.iloc[:,-1].values\n","    tstamps = data_df.time.values\n","\n","    #Calculate the Welch's PSD of the data - yields a smoother, more informative, spectrum \n","    f,pspec = scipy.signal.welch(data_matrix, fs=samp_rate, window='hanning', nperseg=2*samp_rate, noverlap=samp_rate/2,\n","                              nfft=None, detrend='linear', return_onesided=True, scaling='density',axis = 0)\n","\n","    #make figure\n","    nrows = 4\n","    ncols = 2\n","    fig,ax = plt.subplots(nrows,ncols,figsize=(16,10),sharey = True, sharex = True)\n","\n","    for ch in range(data_matrix.shape[1]):\n","        ax[ch//ncols][ch%ncols].loglog(f[1:200*2],pspec[1:200*2,ch])#line noise will be obvious under 100 Hz\n","        #label subplot\n","\n","        ax[ch//ncols][ch%ncols].set_title('Channel %i'%(ch+1))\n","    #label axes\n","    ax[0][0].set_ylabel('Power')\n","    ax[ch//ncols][0].set_xlabel('Frequency')\n","\n","    #despine\n","    sns.despine(fig= plt.gcf(), left = False, right = True, top = True, bottom = False)\n","\n","    fig.tight_layout() \n","\n","    return fig\n","\n","def visualize_time_series_prob(data_folder, prob_class, times, series_labels):\n","    \"\"\"\n","    Visualize probability of each class across time for individual files\n","    \n","    Args:\n","        data_folder: folder with subject data\n","        prob_class: array with probability of each class for each sample\n","        times: array with timestamps for each signal segment\n","        series: array with file of provenance for each singla segment\n","            \n","    Returns:\n","        figure\n","    \"\"\"\n","\n","    #find files in subject folder\n","    file_list = [f for f in os.listdir(data_folder) if os.path.isfile(os.path.join(data_folder, f))]\n","\n","    for file_idx in range(len(file_list)):\n","\n","        # load file\n","        data_df = pd.DataFrame(pd.read_csv(os.path.join(data_folder,file_list[file_idx]), sep='\\t'))\n","\n","        # retrieve start times, end times, and labels for each condition block (will make it easier to parse traces later)\n","        start_times, end_times, event_labels  = get_gesture_times(data_df)\n","\n","        series_idxs = np.where(series_labels==file_idx)[0]\n","\n","        prob_series = prob_class[series_idxs,:]\n","        time_series = times[series_idxs]\n","\n","        classes = np.unique(event_labels)[1:]#exclude 'unmarked' label\n","        labels = []\n","        for c in classes.astype('int'):\n","            labels.append('Class %i'%(c))\n","\n","        # define color palette\n","        palette = sns.color_palette('deep',8)[1:]\n","\n","        plt.figure(figsize=(15,6))\n","        plt.gca().set_prop_cycle(plt.cycler('color',palette))\n","        plt.plot(time_series,prob_series, linewidth = 2);\n","\n","        #mark events\n","        for idx,c in enumerate(event_labels):\n","            if c>0:\n","                plt.hlines(y = 1.1, xmin = start_times[idx], xmax = end_times[idx], color = palette[c-1],linewidth = 10)\n","\n","        #label axes\n","        plt.ylabel('Class Probability')\n","        plt.xlabel('Time (ms)')\n","\n","        plt.legend(labels, bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n","\n","        sns.despine(fig= plt.gcf(), left = False, right = True, top = True, bottom = False)\n","\n","        plt.gcf().tight_layout() \n","\n","        plt.gcf().suptitle('%s'%(file_list[file_idx]), y= 1.05)\n","\n","    return plt.gcf()\n","\n","def dim_reduction_visualization(X, target_labels):\n","    \"\"\"\n","    Perform dimensionality reduction with tSNE and visualize results.\n","    \n","    Args:\n","        X: 2D numpy array with data [samples, features]\n","        target_labels: array. used to color points in embedded space\n"," \n","    Returns:\n","        fig: figure handle\n","    \"\"\"\n","    \n","    #make pipeline\n","    tsne_pipe = make_pipeline(StandardScaler(),#standardize\n","                              TSNE(n_components=2, perplexity = 50))\n","    #perform embedding\n","    X_embedded = tsne_pipe.fit_transform(X)\n","    \n","    # visualize\n","    palette = sns.color_palette('deep',8)\n","\n","    # define class legend\n","    labels = []\n","    custom_lines = []\n","\n","    classes = np.unique(target_labels)#exclude 'unmarked' label\n","    for c in classes.astype('int'):\n","        labels.append('Class %i'%(c))\n","        custom_lines.append(Line2D([0], [0], color=palette[c], lw=4))\n","\n","    sns.set_context('paper',font_scale = 2)\n","    fig,ax = plt.subplots(1,1,figsize=(8,8))\n","    \n","    #plot\n","    for s in range(X_embedded.shape[0]):\n","        ax.scatter(X_embedded[s,0],X_embedded[s,1],\\\n","                        color = palette[int(target_labels[s])],s = 100,linewidth = 2,alpha = 0.8)\n","    ax.axhline(y=0 ,xmin = 0,xmax = 1,color = 'k',linestyle = '--')    \n","    ax.axvline(x=0 ,ymin = 0,ymax = 1,color = 'k',linestyle = '--') \n","    #add legend\n","    ax.legend(custom_lines, labels,bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n","    #label\n","    ax.set_xlabel('Dimension 1')\n","    ax.set_ylabel('Dimension 2')\n","    sns.despine(trim=False, offset=0, bottom=False,top = True, left=False, ax=ax)\n","    \n","    return fig\n","    \n","\n","# ~~~~~~~~ LOGISTIC REGRESSION FUNCTIONS ~~~~~~~~\n","\n","def log_reg_on_all_data(X, Y, nsplits, penalty = 'none', multiclass = 'multinomial',permute = False):\n","    \"\"\"\n","    Train and evaluate a classifier based on logistic regression using all available classes in data\n","    \n","    Args:\n","        X: 2D numpy array with shape [samples, features]\n","        Y: array with class label for each sample\n","        nsplits: number of splits for K-fold cross-validation\n","        permute: Boolean to shuffle class labels (useful to test performance under null hypothesis) \n","        -parameters for logistic regression\n","        penalty: type of penalty for classifier\n","        multiclass: approach for multiclass classification\n","            \n","    Returns:\n","        train_f1_scores: training scores for each split\n","        test_f1_scores: test scores for each split\n","        prob_class: 2D numpy array with probabiliy of each class for each sample\n","    \"\"\"\n","    \n","    #retrieve some values from input\n","    nclass = np.unique(Y).size\n","    nsamples, nfeat = X.shape\n","    \n","    #initialize empty arrays\n","    test_f1_scores =  np.empty((nsplits,))\n","    train_f1_scores =  np.empty((nsplits,))\n","    prob_class = np.empty((nsamples,nclass))\n","    \n","    #permute class labels, if indicated\n","    if permute:\n","        Y = np.random.permutation(Y)\n","    \n","    \n","    #stratify split to retain ratio of class labels\n","    skf = StratifiedKFold(n_splits=nsplits,shuffle = True)\n","\n","\n","    #systematically use one fold of the data as a held-out test set\n","    for split_count, (train_index, test_index) in enumerate(skf.split(X, Y)):\n","\n","        trainX = X[train_index,:]\n","        testX = X[test_index,:]\n","\n","        trainY = Y[train_index]\n","        testY = Y[test_index]\n","\n","        #define model\n","        #note LogisticRegressionCV uses StratifiedKFold by default in cross-validation\n","        model = make_pipeline(StandardScaler(),\\\n","                              LogisticRegression(penalty = penalty, multi_class = multiclass ,max_iter = 10000))\n","        #fit model\n","        model.fit(trainX, trainY)\n","\n","        #predict labels on train set\n","        ypred = model.predict(trainX)\n","        #get F1 score\n","        train_f1_scores[split_count] = f1_score(trainY,ypred,average = 'macro')\n","\n","        #predict labels on test set\n","        ypred = model.predict(testX)\n","        #get F1 score\n","        test_f1_scores[split_count] = f1_score(testY,ypred,average = 'macro')\n","\n","\n","        #get prediction probabiliity on test set samples\n","        pred_prob = model.predict_proba(testX)\n","        prob_class[test_index,:] = pred_prob \n","\n","    return train_f1_scores, test_f1_scores, prob_class\n","\n","def log_reg_on_labeled_data(X, Y, times, series, nsplits, unmarked = 0,penalty = 'none', multiclass = 'multinomial',permute = False):\n","    \"\"\"\n","    Train and evaluate a classifier based on logistic regression using all available classes in data\n","    \n","    Args:\n","        X: 2D numpy array with shape [samples, features]\n","        Y: array with class label for each sample\n","        times: array with timestamps for each signal segment\n","        series: array with file of provenance for each singla segment\n","        nsplits: number of splits for K-fold cross-validation\n","        exclude_label: label to eclude\n","        permute: Boolean to shuffle class labels (useful to test performance under null hypothesis) \n","        -parameters for logistic regression\n","        penalty: type of penalty for classifier\n","        multiclass: approach for multiclass classification\n","            \n","    Returns:\n","        train_f1_scores: training scores for each split\n","        test_f1_scores: test scores for each split\n","        prob_class: 2D numpy array with probabiliy of each class for each sample\n","    \"\"\"\n","    \n","    #change timestamps so that there's no overlapping timestamps across series\n","    times_abs = np.empty((0,))\n","    max_time = 0\n","    for s in np.unique(series):\n","        series_idxs = np.where(series==s)[0]\n","        times_abs = np.hstack((times_abs,times[series_idxs]+max_time))\n","        max_time = np.max(times_abs)\n","\n","    #select \n","    in_samples = np.where(Y != unmarked)[0]\n","    out_samples = np.where(Y == unmarked)[0]\n","    X_in = X[in_samples,:]\n","    Y_in = Y[in_samples]\n","    X_out = X[out_samples,:]\n","    times_in = times_abs[in_samples]\n","    times_out = times_abs[out_samples]\n","    \n","    #retrieve some values from input\n","    nclass = np.unique(Y_in).size\n","    nsamples, nfeat = X_in.shape\n","    \n","    #initialize empty arrays\n","    test_f1_scores =  np.empty((nsplits,))\n","    train_f1_scores =  np.empty((nsplits,))\n","    prob_class_in = np.empty((nsamples,nclass))\n","    prob_class_out = np.empty((nsplits,out_samples.size,nclass))\n","    \n","    #permute class labels, if indicated\n","    if permute:\n","        Y_in = np.random.permutation(Y_in)\n","    \n","    \n","    #stratify split to retain ratio of class labels\n","    skf = StratifiedKFold(n_splits=nsplits,shuffle = True)\n","\n","\n","    #systematically use one fold of the data as a held-out test set\n","    for split_count, (train_index, test_index) in enumerate(skf.split(X_in, Y_in)):\n","\n","        trainX = X_in[train_index,:]\n","        testX = X_in[test_index,:]\n","\n","        trainY = Y_in[train_index]\n","        testY = Y_in[test_index]\n","\n","        #define model\n","        #note LogisticRegressionCV uses StratifiedKFold by default in cross-validation\n","        model = make_pipeline(StandardScaler(),\\\n","                              LogisticRegression(penalty = penalty, multi_class = multiclass ,max_iter = 10000))\n","        #fit model\n","        model.fit(trainX, trainY)\n","\n","        #predict labels on train set\n","        ypred = model.predict(trainX)\n","        #get F1 score (weighted to account for slight class imbalance)\n","        train_f1_scores[split_count] = f1_score(trainY,ypred,average = 'weighted')\n","\n","        #predict labels on test set\n","        ypred = model.predict(testX)\n","        #get F1 score (weighted to account for slight class imbalance)\n","        test_f1_scores[split_count] = f1_score(testY,ypred,average = 'weighted')\n","\n","\n","        #get prediction probabiliity on test set samples\n","        pred_prob = model.predict_proba(testX)\n","        prob_class_in[test_index,:] = pred_prob \n","        \n","        #get prediction probability on 'unmarked' samples\n","        pred_prob = model.predict_proba(X_out)\n","        prob_class_out[split_count,:,:] = pred_prob\n","    \n","    #get probability of each class over time\n","    # average over multiple splits\n","    prob_class_out = np.mean(prob_class_out,0)\n","    \n","    #concatenate and sort, using window timestamps as a guide\n","    T_all = np.hstack((times_in,times_out))\n","    prob_class = np.vstack((prob_class_in,prob_class_out))\n","    sort_idxs = np.argsort(T_all)\n","    prob_class = prob_class[sort_idxs,:]\n","    \n","    return train_f1_scores, test_f1_scores, prob_class\n","\n","# ~~~~~~~~ RNN CLASSIFIER FUNCTIONS ~~~~~~~~\n","def get_data_cube(X, Y, window_blocks, train = True, scaler = None, magic_value = -100):\n","    \"\"\"\n","    Create data cube for use with Keras RNN. Standardize data then pad and reshape data to have\n","    [samples, timesteps, features] dimensions with an equal number of timesteps for each slice\n","    I use a Masking layer in the RNN architecture to allow for sequences of different length\n","    \n","    Args:\n","        X: 2D nuumpy array with data, dimensions [features, samples]\n","        window_blocks: 1D numpy array indicating block of provenance for input segment values\n","        train: Boolean indicating whether the input data is training data\n","        scaler: StandardScaler to transform data\n","        magic_value: integer indicating value with which to pad samples\n","            \n","    Returns:\n","        X_cube: 3D numpy array of size [samples, timesteps, features]\n","        scaler: 1D numpy array with class label for each segment\n","    \"\"\"\n","    #standardize across each feature dimension\n","    if train:\n","        scaler = StandardScaler()\n","        scaler = scaler.fit(X.T)\n","        X = scaler.transform(X.T).T\n","    else:\n","        #for testing data, we want to use same transform as was fit to training data\n","        X = scaler.transform(X.T).T\n","\n","    # common number of time steps\n","    common_timesteps = np.max(np.bincount(window_blocks.astype('int')))\n","    \n","    # get each block, pad, and stack to form a data cube\n","    X_cube = []\n","    Y_cube = []\n","    for b_count, b_idx in enumerate(np.unique(window_blocks)):\n","       #slice\n","        X_slice = X[:,np.where(window_blocks==b_idx)[0]]\n","        Y_slice = Y[np.where(window_blocks==b_idx)[0],:].T\n","        #pad - can just use keras padding function\n","        pad_size = common_timesteps-X_slice.shape[1]\n","        X_slice_pad = np.pad(X_slice,pad_width=((0,0),(0,pad_size)), mode='constant', constant_values= magic_value)\n","        Y_slice_pad = np.pad(Y_slice,pad_width=((0,0),(0,pad_size)), mode='constant', constant_values= 0)\n","        #stack\n","        if b_count == 0:\n","            X_cube  = X_slice_pad\n","            Y_cube = Y_slice_pad\n","        else:\n","            X_cube = np.dstack((X_cube,X_slice_pad))\n","            Y_cube = np.dstack((Y_cube,Y_slice_pad))\n","    # swap dimension to get [samples, timesteps, features]\n","    X_cube = np.swapaxes(X_cube,0,2)\n","    Y_cube = np.swapaxes(Y_cube,0,2)\n","    \n","    return X_cube, Y_cube, scaler\n","\n","def many_to_many_model(input_shape, n_outputs, mask_value = -100):\n","    \"\"\"\n","    Create simple RNN model\n","    \n","    Args:\n","        input_shape\n","        n_outputs: number of output classes\n","        mask_value: value indicating which timepoints to mask out\n","            \n","    Returns:\n","        model\n","    \"\"\"\n","    \n","    #define model architecture\n","    X_input = Input(shape = input_shape)\n","    X = Masking(mask_value=mask_value)(X_input)\n","    X = GRU(24, return_sequences= True, stateful = False)(X)\n","    X = Dropout(0.8)(X)\n","    X = TimeDistributed(Dense(n_outputs,activation = 'softmax'))(X)\n","    model = Model(inputs = X_input, outputs = X)\n","    return model\n","\n","def RNN_on_labeled_data(feature_matrix, target_labels, window_tstamps, block_labels, n_splits = 4,\\\n","                       verbose = 0, epochs = 40, batch_size = 2, permute = False):\n","    \"\"\"\n","    Train and evaluate RNN model on labeled data\n","    \n","    Args:\n","        feature_matrix: 2D nuumpy array with data, dimensions [features, samples]\n","        window_blocks: 1D numpy array indicating block of provenance for input segment values\n","        train: Boolean indicating whether the input data is training data\n","        scaler: StandardScaler to transform data\n","        magic_value: integer indicating value with which to pad samples\n","            \n","    Returns:\n","        train_f1_scores: training scores for each split\n","        test_f1_scores: test scores for each split\n","        train_loss: training loss metric for each split\n","        test_loss_scores: test loss metric for each split\n","\n","    \"\"\"\n","    \n","    # transpose data\n","    #feature_matrix = feature_matrix.T\n","    \n","    #initialize empty array\n","    train_f1_scores = np.empty((n_splits,))\n","    test_f1_scores = np.empty((n_splits,))\n","    train_loss = np.empty((n_splits,))\n","    test_loss = np.empty((n_splits,))\n","\n","    #get block_ids and corresponding classes in block. there are the units over which we will do train/test split\n","    blocks = np.array([k for k,g in groupby(block_labels) if k!=0])\n","    classes = np.array([k for k,g in groupby(target_labels) if k!=0])\n","    \n","    #permute class labels, if indicated\n","    if permute:\n","        #using indexing tricks to have this work out\n","        classes_perm = np.random.permutation(classes)\n","        target_labels_shuffled = np.empty((0,))\n","        for i,b in enumerate(blocks):\n","            idxs = np.where(block_labels==b)[0]\n","            target_labels_shuffled = np.hstack((target_labels_shuffled,classes_perm[i]*np.ones((idxs.size,))))\n","        target_labels = target_labels_shuffled\n","        classes = classes_perm\n","     \n","\n","    #stratify split to retain ratio of class labels\n","    skf = StratifiedKFold(n_splits=n_splits,shuffle = True)\n","\n","    #systematically use one fold of the data as a held-out test set\n","    for split_count, (blocks_train_idxs, blocks_test_idxs) in enumerate(skf.split(blocks, classes)):\n","        print('Split Count: %i'% (split_count+1))\n","\n","        #get train and test indices\n","        blocks_train = blocks[blocks_train_idxs]\n","        blocks_test = blocks[blocks_test_idxs]\n","        train_idxs =np.where(np.isin(block_labels,blocks_train))[0]\n","        test_idxs =np.where(np.isin(block_labels,blocks_test))[0]\n","\n","        # select training data and pad to get an array where each sample has same number of timesteps\n","        X_train = feature_matrix[:,train_idxs]\n","        y_train = target_labels[train_idxs]\n","        #one-hot encoding of class labels\n","        y_train = to_categorical(y_train-np.min(y_train))\n","        #get block labels of given samples\n","        win_blocks_train = block_labels[train_idxs]\n","\n","        #get cube\n","        X_train_cube, Y_train_cube, scaler = get_data_cube(X_train, y_train,win_blocks_train, train = True, magic_value = -100)\n","        print(X_train_cube.shape, Y_train_cube.shape)\n","\n","        # select test data and pad to get an array where each sample has same number of timesteps\n","        X_test = feature_matrix[:,test_idxs]\n","        y_test = target_labels[test_idxs]\n","        #one-hot encoding of class labels\n","        y_test = to_categorical(y_test-np.min(y_test))\n","        #get block labels of given samples\n","        win_blocks_test = block_labels[test_idxs]\n","        #get data cube\n","        X_test_cube, Y_test_cube, scaler = get_data_cube(X_test, y_test, win_blocks_test, train = False, scaler = scaler, magic_value = -100)\n","        print(X_test_cube.shape, Y_test_cube.shape)\n","\n","        n_timesteps, n_features, n_outputs = X_train_cube.shape[1], X_train_cube.shape[2], Y_test_cube.shape[2]\n","\n","        #setting timestep dimension to None \n","        model = many_to_many_model((None,n_features),n_outputs,mask_value = -100)\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',Precision(), Recall()])\n","        #model.summary\n","\n","        print('Training Model')\n","        # fit network\n","        model.fit(X_train_cube, Y_train_cube, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","\n","        print('Evaluating Model')\n","\n","        #evaluate on trained data\n","        loss, accuracy, precision, recall = model.evaluate(X_train_cube, Y_train_cube, batch_size=batch_size, verbose=verbose)\n","        \n","        #compute f1 score and store\n","        f1_score = 2* ((precision * recall)/(precision + recall))\n","        train_f1_scores[split_count] = f1_score\n","        train_loss[split_count] = loss\n","\n","        #evaluate on trained data\n","        loss, accuracy, precision, recall = model.evaluate(X_test_cube, Y_test_cube, batch_size=batch_size, verbose=verbose)\n","        #compute f1 score and store\n","        f1_score = 2* ((precision * recall)/(precision + recall))\n","        test_f1_scores[split_count] = f1_score\n","        test_loss[split_count] = loss\n","    return train_f1_scores, test_f1_scores, train_loss, test_loss"],"id":"0D-4n81T3MgH","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"273598a7"},"source":["def get_RNN_f1(X, Y, model, average = 'weighted', mask_value = -100):\n","    \"\"\"\n","    Get f1 score for an RNN model using masked timepoint data\n","\n","    Args:\n","        X: 3D numpy array with shape [samples, timepoints, features]\n","        Y: 3D numpy array with shape [samples, timepoints, classes]. one-hot coding of classes\n","        model: RNN model object\n","        average: string argument for f1_score function. Usually 'macro' or 'weighted'\n","        mask_value: value indicating which timepoints to mask out\n","\n","    Returns:\n","        f1: f1 score\n","    \"\"\"\n","    # Mask out indices based on mask value\n","    nonmasked_idxs = np.where(X[:,:,0].flatten()!=mask_value)[0]\n","    # Get target labels for non-masked timepoints\n","    y_true = np.argmax(Y,2).flatten()[nonmasked_idxs]\n","    # Get model predictions for non-masked timepoints\n","    preds = model.predict(X)\n","    y_pred = np.argmax(preds,2).flatten()[nonmasked_idxs]\n","    # Get F1 score\n","    f1 = f1_score(y_true,y_pred,average = average)\n","\n","    return f1"],"id":"273598a7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4324f46","executionInfo":{"status":"ok","timestamp":1624033534076,"user_tz":240,"elapsed":16,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"6187490b-8c7c-4197-c8ff-d0df5e4d9fe0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"b4324f46","execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cXN_vrZP3wJx"},"source":["\n","\n","n_splits = 4\n","verbose = 0\n","epochs = 10\n","n_epoch_stretch = 10\n","\n","batch_size = 2\n","permute = False\n","\n","#empty arrays\n","train_f1_scores = np.empty((n_splits,n_epoch_stretch,2))\n","test_f1_scores = np.empty((n_splits,n_epoch_stretch,2))\n","\n","#define where the data file are located\n","data_folder = '/content/drive/MyDrive/EMG_data/01'\n","\n","\n","#filter parameters\n","lo_freq = 20\n","hi_freq = 450\n","\n","win_size = 100 #define window size over which to compute time-domain features\n","step = win_size #keeping this parameter in case I want to re-run later with some overlap\n"],"id":"cXN_vrZP3wJx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4ed8713","executionInfo":{"status":"ok","timestamp":1624033644120,"user_tz":240,"elapsed":106088,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"f0626534-81a6-4c12-c0f6-5e9e70b5f25f"},"source":["for perm_idx, permute in enumerate([False,True]):\n","\n","  #get features across segments and corresponding info\n","  feature_matrix, target_labels, window_tstamps, \\\n","  block_labels, series_labels = get_subject_data_for_classification(data_folder, lo_freq, hi_freq, \\\n","                                                                    win_size, step)\n","\n","  #exclude blocks with 'unknown' label\n","  in_samples = np.where(target_labels != 0)[0]\n","  feature_matrix_in = feature_matrix[in_samples,:]\n","  target_labels_in = target_labels[in_samples]\n","  window_tstamps_in = window_tstamps[in_samples]\n","  block_labels_in = block_labels[in_samples]\n","\n","\n","  feature_matrix = feature_matrix_in.T\n","  target_labels = target_labels_in\n","  window_tstamps_ = window_tstamps_in\n","  block_labels = block_labels_in\n","\n","\n","\n","  #get block_ids and corresponding classes in block. there are the units over which we will do train/test split\n","  blocks = np.array([k for k,g in groupby(block_labels) if k!=0])\n","  classes = np.array([k for k,g in groupby(target_labels) if k!=0])\n","\n","  #permute class labels, if indicated\n","  if permute:\n","      #using indexing tricks to have this work out\n","      classes_perm = np.random.permutation(classes)\n","      target_labels_shuffled = np.empty((0,))\n","      for i,b in enumerate(blocks):\n","          idxs = np.where(block_labels==b)[0]\n","          target_labels_shuffled = np.hstack((target_labels_shuffled,classes_perm[i]*np.ones((idxs.size,))))\n","      target_labels = target_labels_shuffled\n","      classes = classes_perm\n","\n","\n","  #stratify split to retain ratio of class labels\n","  skf = StratifiedKFold(n_splits=n_splits,shuffle = True)\n","\n","  #systematically use one fold of the data as a held-out test set\n","  for split_count, (blocks_train_idxs, blocks_test_idxs) in enumerate(skf.split(blocks, classes)):\n","      print('Split Count: %i'% (split_count+1))\n","\n","      #get train and test indices\n","      blocks_train = blocks[blocks_train_idxs]\n","      blocks_test = blocks[blocks_test_idxs]\n","      train_idxs =np.where(np.isin(block_labels,blocks_train))[0]\n","      test_idxs =np.where(np.isin(block_labels,blocks_test))[0]\n","\n","      # select training data and pad to get an array where each sample has same number of timesteps\n","      X_train = feature_matrix[:,train_idxs]\n","      y_train = target_labels[train_idxs]\n","      #one-hot encoding of class labels\n","      y_train = to_categorical(y_train-np.min(y_train))\n","      #get block labels of given samples\n","      win_blocks_train = block_labels[train_idxs]\n","\n","      #get cube\n","      X_train_cube, Y_train_cube, scaler = get_data_cube(X_train, y_train,win_blocks_train, train = True, magic_value = -100)\n","      print(X_train_cube.shape, Y_train_cube.shape)\n","\n","      # select test data and pad to get an array where each sample has same number of timesteps\n","      X_test = feature_matrix[:,test_idxs]\n","      y_test = target_labels[test_idxs]\n","      #one-hot encoding of class labels\n","      y_test = to_categorical(y_test-np.min(y_test))\n","      #get block labels of given samples\n","      win_blocks_test = block_labels[test_idxs]\n","      #get data cube\n","      X_test_cube, Y_test_cube, scaler = get_data_cube(X_test, y_test, win_blocks_test, train = False, scaler = scaler, magic_value = -100)\n","      print(X_test_cube.shape, Y_test_cube.shape)\n","\n","      n_timesteps, n_features, n_outputs = X_train_cube.shape[1], X_train_cube.shape[2], Y_test_cube.shape[2]\n","\n","      #setting timestep dimension to None for variable length input\n","      model = many_to_many_model((None,n_features),n_outputs,mask_value = -100)\n","      model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","      #model.summary\n","      for stretch in range(n_epoch_stretch):\n","        print(stretch)\n","        #fit model\n","        model.fit(X_train_cube, Y_train_cube, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","\n","        #evaluate model\n","        train_f1_scores[split_count, stretch, perm_idx] = get_RNN_f1(X_train_cube, Y_train_cube, model)\n","        test_f1_scores[split_count, stretch, perm_idx] = get_RNN_f1(X_test_cube, Y_test_cube, model)\n","\n"],"id":"a4ed8713","execution_count":null,"outputs":[{"output_type":"stream","text":["Split Count: 1\n","(18, 21, 16) (18, 21, 6)\n","(6, 20, 16) (6, 20, 6)\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","Split Count: 2\n","(18, 21, 16) (18, 21, 6)\n","(6, 20, 16) (6, 20, 6)\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","Split Count: 3\n","(18, 21, 16) (18, 21, 6)\n","(6, 18, 16) (6, 18, 6)\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","Split Count: 4\n","(18, 20, 16) (18, 20, 6)\n","(6, 21, 16) (6, 21, 6)\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","Split Count: 1\n","(18, 20, 16) (18, 20, 6)\n","(6, 21, 16) (6, 21, 6)\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","Split Count: 2\n","(18, 21, 16) (18, 21, 6)\n","(6, 18, 16) (6, 18, 6)\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","Split Count: 3\n","(18, 21, 16) (18, 21, 6)\n","(6, 18, 16) (6, 18, 6)\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","Split Count: 4\n","(18, 21, 16) (18, 21, 6)\n","(6, 20, 16) (6, 20, 6)\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e6abf316"},"source":["train_f1_mean = np.mean(train_f1_scores,0)\n","test_f1_mean = np.mean(test_f1_scores,0)"],"id":"e6abf316","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DpiO0sqT5nyc","executionInfo":{"status":"ok","timestamp":1624033656043,"user_tz":240,"elapsed":146,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"6fea418f-228f-420c-a924-7c9d063e067d"},"source":["test_f1_mean[:,0]"],"id":"DpiO0sqT5nyc","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.71317878, 0.87612743, 0.93132112, 0.95214068, 0.95685081,\n","       0.96226563, 0.96521831, 0.97341574, 0.96232135, 0.96101242])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291},"id":"4327d163","executionInfo":{"status":"ok","timestamp":1624033669455,"user_tz":240,"elapsed":339,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"21bf245f-0855-41af-d223-d414d00c39a7"},"source":["plt.plot(train_f1_mean[:,0],'b')\n","plt.plot(test_f1_mean[:,0],'r')\n","plt.plot(train_f1_mean[:,1],'--b')\n","plt.plot(test_f1_mean[:,1],'--r')"],"id":"4327d163","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f6d438d2b90>]"]},"metadata":{"tags":[]},"execution_count":32},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"3ddd014a","outputId":"803226be-d1d9-423a-f1ef-58b8e0efaa40"},"source":["plt.plot(train_f1_mean[:,0],'b')\n","plt.plot(test_f1_mean[:,0],'r')"],"id":"3ddd014a","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3.95252517e-323, 4.44659081e-323, 4.94065646e-323, 5.74877094e-001])"]},"metadata":{"tags":[]},"execution_count":123}]},{"cell_type":"code","metadata":{"id":"03725ab2","outputId":"2ddc2fb0-8bbc-4c39-81a4-310cb9e671e7"},"source":["Y.shape"],"id":"03725ab2","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6, 18, 6)"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"469edd98"},"source":["#get f1 score for train and test data\n","#seem to overfitting, find good # of epochs to get perm lower"],"id":"469edd98","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"71e5bcf9"},"source":["Y = Y_test_cube.copy()\n","X = X_test_cube.copy()\n","# Y = Y_train_cube.copy()\n","# X = X_train_cube.copy()"],"id":"71e5bcf9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"70657363"},"source":["preds = model.predict(X)"],"id":"70657363","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"56e29269"},"source":["mask_value = -100\n","\n","nonmasked_idxs = np.where(X[:,:,0].flatten()!=mask_value)[0]\n","y_true = np.argmax(Y,2).flatten()[nonmasked_idxs]\n","y_pred = np.argmax(preds,2).flatten()[nonmasked_idxs]\n","\n","\n"],"id":"56e29269","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7eb5137b","outputId":"9dc222cf-7431-410f-dfc7-6aa0a2712f87"},"source":["f1_score(y_true,y_pred,average = 'weighted')"],"id":"7eb5137b","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.31077357624083624"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"08248eec","outputId":"a9e15238-fd5f-447e-9f60-8324017ef662"},"source":["Y_train_cube[1,:,0]"],"id":"08248eec","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0.], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"c8611af1","outputId":"0206063e-84bf-4eed-d2ba-51fbb0f651e8"},"source":["np.argmax(preds,2)[1,:]"],"id":"c8611af1","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"9b68e158","outputId":"a8dbb746-c9c9-4bbf-c0fa-5111487b7c97"},"source":["preds[0,0,:]"],"id":"9b68e158","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.01387087, 0.56795704, 0.01889889, 0.17304797, 0.06072921,\n","       0.16549602], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"eeba2ae0"},"source":["#setting timestep dimension to None \n","model = many_to_many_model((None,n_features),n_outputs,mask_value = -100)\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',Precision(), Recall()])\n","#model.summary"],"id":"eeba2ae0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0a06a975"},"source":[""],"id":"0a06a975","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"acee0a3a"},"source":["preds = model.predict(X_train_cube)"],"id":"acee0a3a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f14e21f0"},"source":["np.argmax(preds,0)"],"id":"f14e21f0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4b4f6ceb"},"source":["model."],"id":"4b4f6ceb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f34f79dc"},"source":["        #setting timestep dimension to None \n","        model = many_to_many_model((None,n_features),n_outputs,mask_value = -100)\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',Precision(), Recall()])\n","        #model.summary\n","\n","        print('Training Model')\n","        # fit network\n","        model.fit(X_train_cube, Y_train_cube, epochs=epochs, batch_size=batch_size, verbose=verbose)"],"id":"f34f79dc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b05efff0"},"source":["    #initialize empty array\n","    train_f1_scores = np.empty((n_splits,))\n","    test_f1_scores = np.empty((n_splits,))\n","    train_loss = np.empty((n_splits,))\n","    test_loss = np.empty((n_splits,))\n","\n","    #get block_ids and corresponding classes in block. there are the units over which we will do train/test split\n","    blocks = np.array([k for k,g in groupby(block_labels) if k!=0])\n","    classes = np.array([k for k,g in groupby(target_labels) if k!=0])\n","    \n","    #permute class labels, if indicated\n","    if permute:\n","        #using indexing tricks to have this work out\n","        classes_perm = np.random.permutation(classes)\n","        target_labels_shuffled = np.empty((0,))\n","        for i,b in enumerate(blocks):\n","            idxs = np.where(block_labels==b)[0]\n","            target_labels_shuffled = np.hstack((target_labels_shuffled,classes_perm[i]*np.ones((idxs.size,))))\n","        target_labels = target_labels_shuffled\n","        classes = classes_perm\n","     \n","\n","    #stratify split to retain ratio of class labels\n","    skf = StratifiedKFold(n_splits=n_splits,shuffle = True)\n","\n","    #systematically use one fold of the data as a held-out test set\n","    for split_count, (blocks_train_idxs, blocks_test_idxs) in enumerate(skf.split(blocks, classes)):\n","        print('Split Count: %i'% (split_count+1))\n","\n","        #get train and test indices\n","        blocks_train = blocks[blocks_train_idxs]\n","        blocks_test = blocks[blocks_test_idxs]\n","        train_idxs =np.where(np.isin(block_labels,blocks_train))[0]\n","        test_idxs =np.where(np.isin(block_labels,blocks_test))[0]\n","\n","        # select training data and pad to get an array where each sample has same number of timesteps\n","        X_train = feature_matrix[:,train_idxs]\n","        y_train = target_labels[train_idxs]\n","        #one-hot encoding of class labels\n","        y_train = to_categorical(y_train-np.min(y_train))\n","        #get block labels of given samples\n","        win_blocks_train = block_labels[train_idxs]\n","\n","        #get cube\n","        X_train_cube, Y_train_cube, scaler = get_data_cube(X_train, y_train,win_blocks_train, train = True, magic_value = -100)\n","        print(X_train_cube.shape, Y_train_cube.shape)\n","\n","        # select test data and pad to get an array where each sample has same number of timesteps\n","        X_test = feature_matrix[:,test_idxs]\n","        y_test = target_labels[test_idxs]\n","        #one-hot encoding of class labels\n","        y_test = to_categorical(y_test-np.min(y_test))\n","        #get block labels of given samples\n","        win_blocks_test = block_labels[test_idxs]\n","        #get data cube\n","        X_test_cube, Y_test_cube, scaler = get_data_cube(X_test, y_test, win_blocks_test, train = False, scaler = scaler, magic_value = -100)\n","        print(X_test_cube.shape, Y_test_cube.shape)\n","\n","        n_timesteps, n_features, n_outputs = X_train_cube.shape[1], X_train_cube.shape[2], Y_test_cube.shape[2]\n","\n","        #setting timestep dimension to None \n","        model = many_to_many_model((None,n_features),n_outputs,mask_value = -100)\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',Precision(), Recall()])\n","        #model.summary\n","\n","        print('Training Model')\n","        # fit network\n","        model.fit(X_train_cube, Y_train_cube, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","\n","        print('Evaluating Model')\n","\n","        #evaluate on trained data\n","        loss, accuracy, precision, recall = model.evaluate(X_train_cube, Y_train_cube, batch_size=batch_size, verbose=verbose)\n","        "],"id":"b05efff0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"206942fa"},"source":[""],"id":"206942fa","execution_count":null,"outputs":[]}]}