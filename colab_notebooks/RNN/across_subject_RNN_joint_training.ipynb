{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"across_subject_RNN_joint_training.ipynb","provenance":[],"authorship_tag":"ABX9TyNhfSbWEc/cqRmUjT0DUET0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"cXvDXR3rJjK8"},"source":["#Run cell to mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PEcl5H5YJtXS"},"source":["# install package to have access to custom functions\n","%pip install /content/drive/MyDrive/EMG_gestures/ --use-feature=in-tree-build"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCFRYRsRJxx4"},"source":["#import necessary packages\n","\n","#our workhorses\n","import numpy as np\n","import pandas as pd\n","import scipy\n","\n","#to visualize\n","%matplotlib inline\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","#style params for figures\n","sns.set(font_scale = 2)\n","plt.style.use('seaborn-white')\n","plt.rc(\"axes\", labelweight=\"bold\")\n","from IPython.display import display, HTML\n","\n","#to load files\n","import os\n","import sys\n","import h5py\n","import pickle\n","from tensorflow import keras\n","\n","#append repo folder to search path\n","#import cusotm functions\n","from EMG_gestures.utils import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ly6n0VzMJzPP"},"source":["#define hyper params for each model\n","model_dict = {1:{'n_grus':24, 'n_dense_pre':0, 'n_dense_post': 0, 'activation':''},\\\n","              2:{'n_grus':24, 'n_dense_pre':1, 'n_dense_post': 0, 'activation':'tanh'},\\\n","              3:{'n_grus':24, 'n_dense_pre':2, 'n_dense_post': 0, 'activation':'tanh'},\\\n","              4:{'n_grus':24, 'n_dense_pre':3, 'n_dense_post': 0, 'activation':'tanh'},\\\n","              5:{'n_grus':24, 'n_dense_pre':1, 'n_dense_post': 0, 'activation':'relu'},\\\n","              6:{'n_grus':24, 'n_dense_pre':2, 'n_dense_post': 0, 'activation':'relu'},\\\n","              7:{'n_grus':24, 'n_dense_pre':3, 'n_dense_post': 0, 'activation':'relu'},\\\n","              8:{'n_grus':24, 'n_dense_pre':0, 'n_dense_post': 1, 'activation':'tanh'},\\\n","              9:{'n_grus':24, 'n_dense_pre':0, 'n_dense_post': 2, 'activation':'tanh'},\\\n","              10:{'n_grus':24, 'n_dense_pre':0, 'n_dense_post': 3, 'activation':'tanh'},\\\n","              11:{'n_grus':24, 'n_dense_pre':0, 'n_dense_post': 1, 'activation':'relu'},\\\n","              12:{'n_grus':24, 'n_dense_pre':0, 'n_dense_post': 2, 'activation':'relu'},\\\n","              13:{'n_grus':24, 'n_dense_pre':0, 'n_dense_post': 3, 'activation':'relu'},\\\n","              14:{'n_grus':24, 'n_dense_pre':2, 'n_dense_post': 2, 'activation':'tanh'},\\\n","              15:{'n_grus':24, 'n_dense_pre':2, 'n_dense_post': 2, 'activation':'relu'},\\\n","              }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VN4XFSvnJ2ow"},"source":["#define where the data files are located\n","data_folder = '/content/drive/MyDrive/EMG_gestures/EMG_data/'\n","\n","nsubjects = 36\n","\n","#randomly-selected subjects to use as hold-out test data \n","test_subjects = [17, 23,  7,  8,  3]\n","\n","# User-defined parameters\n","lo_freq = 20 #lower bound of bandpass filter\n","hi_freq = 450 #upper bound of bandpass filter\n","\n","win_size = 100 #define window size over which to compute time-domain features\n","step = win_size #keeping this parameter in case we want to re-run later with some overlap"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7RJ8uRD_J6Qd"},"source":["#intialize empty lists\n","feature_matrix_all = np.empty((0,0))\n","target_labels_all = np.empty((0,))\n","window_tstamps_all = np.empty((0,))\n","block_labels_all  = np.empty((0,))\n","subject_id_all = np.empty((0,))\n","block_count = 0\n","\n","for subject_id in range(1,nsubjects+1):\n","    if subject_id not in test_subjects:\n","        subject_folder = os.path.join(data_folder,'%02d'%(subject_id))\n","        print('=======================')\n","        print(subject_folder)\n","\n","        # Process data and get features \n","        #get features across segments and corresponding info\n","        feature_matrix, target_labels, window_tstamps, \\\n","        block_labels, series_labels = get_subject_data_for_classification(subject_folder, lo_freq, hi_freq, \\\n","                                                                        win_size, step)\n","\n","        #prevent repeat of block labels by increasing block count\n","        block_labels = block_labels+block_count\n","        block_count = np.max([block_count, np.max(block_labels)])\n","\n","        # concatenate lists\n","        feature_matrix_all = np.vstack((feature_matrix_all,feature_matrix)) if feature_matrix_all.size else feature_matrix\n","        target_labels_all = np.hstack((target_labels_all,target_labels))\n","        window_tstamps_all = np.hstack((window_tstamps_all,window_tstamps))\n","        block_labels_all = np.hstack((block_labels_all,block_labels))\n","        subject_id_all = np.hstack((subject_id_all,np.ones((block_labels.size))*subject_id))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAZIQ7cUKJm_"},"source":["results_folder = '/content/drive/MyDrive/EMG_gestures/results_data/xsubject_joint_data/RNN/'\n","figure_folder = '/content/drive/MyDrive/EMG_gestures/figures/training_history/xsubject_joint_data/RNN'\n","\n","\n","#RNN training args - all other arguments are the same\n","verbose = 0\n","epochs = 30\n","batch_size = 5\n","validation_split = 0.1\n","# experiment params\n","n_splits = 4\n","nsets_training = 10\n","nreps = 10\n","\n","#excluded labels\n","exclude = [0,7]\n","#performance metrics\n","score_list = ['f1','accuracy']\n","\n","model_id = 1\n","rep = 0\n","#for model_id in range(1,5+1):\n","np.random.seed(1)# Set seed for replicability\n","results_df = []\n","   # for rep in range(nreps):\n","print('Model %d | Rep %d'%(model_id, rep+1))\n","print('--True Data--')\n","rep_results_df, train_history = log_reg_rnn_joint_data_train_frac_subjects(feature_matrix_all, target_labels_all, subject_id_all,\\\n","                                                                                block_labels_all, exclude,\\\n","                                                                                model_dict = model_dict[model_id], score_list = score_list,\\\n","                                                                                n_splits = n_splits, nsets_training = nsets_training,\\\n","                                                                                verbose = verbose, epochs = epochs, batch_size = batch_size,\\\n","                                                                                validation_split = validation_split, mv = None, permute = False)\n","    #     #add details and concatenate dataframe\n","    #     rep_results_df['Shuffled'] = False\n","    #     rep_results_df['Rep'] =  rep+1\n","    #     results_df.append(rep_results_df)\n","\n","\n","    #     #plot training history\n","    #     fig_title = 'Log reg model %02d; rep %i'%(model_id,rep)\n","    #     fig_fn = os.path.join(figure_folder,'log_reg_model_%02d_rep_%i_loss.png'%(model_id,rep))\n","    #     plot_training_history(train_history, fig_title,fig_fn)\n","\n","    #     #repeat with shuffled data\n","    #     print('Model %d | Rep %d'%(model_id, rep+1))\n","    #     print('--Permuted Data--')\n","    #     rep_results_df, train_history = log_reg_xsubject_joint_data_train_frac_subjects(feature_matrix_all, target_labels_all, subject_id_all,\\\n","    #                                                                                     block_labels_all, exclude,\\\n","    #                                                                                     model_dict = model_dict[model_id], score_list = score_list,\\\n","    #                                                                                     n_splits = n_splits, nsets_training = nsets_training,\\\n","    #                                                                                     verbose = verbose, epochs = epochs, batch_size = batch_size,\\\n","    #                                                                                     validation_split = validation_split, mv = None, permute = True)\n","    #     #add details and concatenate dataframe\n","    #     rep_results_df['Shuffled'] = True\n","    #     rep_results_df['Rep'] =  rep+1\n","    #     results_df.append(rep_results_df)\n","\n","    # #concatenate all data frames\n","    # results_df = pd.concat(results_df,axis = 0)\n","\n","    # # #save results to file\n","    # results_fn = 'model_%02d_results.h5'%(model_id)\n","    # results_df.to_hdf(os.path.join(results_folder,results_fn), key='results_df', mode='w')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KFnUruKPKJra"},"source":["# def rnn_xsubject_joint_data_train_frac_subjects(feature_matrix, target_labels, sub_labels, block_labels, exclude,\\\n","#                                                     model_dict, score_list, n_splits = 4, nsets_training = 10,\\\n","#                                                     verbose = 0, epochs = 40, batch_size = 2, validation_split = 0.1, mv = False, permute = False):\n","#     \"\"\"\n","#     train and validate a RNN model using data from multiple subjects \n","#     train and validate model performance by splitting subjects into a train and test set\n","#     \"\"\"\n","\n","#subjects in list. there are the units over which we will do train/test split\n","subs = np.unique(sub_labels)\n","\n","if permute:\n","    #permute while ignoring excluded blocks\n","    target_labels = permute_class_within_sub(target_labels, block_labels, sub_labels, exclude)\n","\n","\n","#initialize object for k-fold cross-validation\n","kf = KFold(n_splits=n_splits,shuffle = True)\n","#initialize empty arrays\n","\n","n_scores = len(score_list)\n","train_scores_all = np.empty((n_splits,n_scores))\n","test_scores_all = np.empty((n_splits,n_scores))\n","train_history = dict()\n","train_history['loss'] = np.empty((0,0))\n","train_history['val_loss'] = np.empty((0,0))\n","\n","for split_count, (subs_train_idxs, subs_test_idxs) in enumerate(kf.split(subs)):\n","    print('Split Count: %i'% (split_count+1))\n","\n","    #get train and test indices\n","    train_subs = subs[subs_train_idxs]\n","    test_subs = subs[subs_test_idxs]\n","    train_idxs = np.where(np.isin(sub_labels,train_subs, invert = False))[0]\n","    test_idxs = np.where(np.isin(sub_labels,test_subs, invert = False))[0]\n","\n","    #get trained model\n","    train_scores, trained_model, scaler, history = get_trained_rnn_model(feature_matrix, target_labels, train_idxs, block_labels, nsets_training,\\\n","                                                                        exclude, model_dict, score_list,\\\n","                                                                        verbose = verbose, epochs = epochs, batch_size = batch_size,\\\n","                                                                        validation_split = validation_split,\\\n","                                                                        mv = mv)\n","    #Evaluating on held-out subjects\n","    test_scores = evaluate_trained_rnn(feature_matrix, target_labels, test_idxs, exclude, trained_model, score_list,scaler, mv = mv)\n","\n","    #put scores in array\n","    train_scores_all[split_count,:] = train_scores\n","    test_scores_all[split_count,:] = test_scores\n","\n","    #append history\n","    train_history['loss'] = np.vstack((train_history['loss'],history.history['loss'])) if train_history['loss'].size else np.array(history.history['loss'])\n","    if validation_split>0:\n","        train_history['val_loss'] = np.vstack((train_history['val_loss'],history.history['val_loss'])) if train_history['val_loss'].size else np.array(history.history['val_loss']) \n","\n","#put in data frame\n","results_df = []\n","data_dict = {'Fold':np.arange(n_splits)+1,\\\n","                'Type':['Train' for x in range(n_splits)]}\n","for sidx in range(n_scores):\n","    data_dict['%s_score'%(score_list[sidx])] = train_scores_all[:,sidx]\n","results_df.append(pd.DataFrame(data_dict))\n","\n","data_dict = {'Fold':np.arange(n_splits)+1,\\\n","                'Type':['Test' for x in range(n_splits)]}\n","for sidx in range(n_scores):\n","    data_dict['%s_score'%(score_list[sidx])] = test_scores_all[:,sidx]\n","results_df.append(pd.DataFrame(data_dict))\n","\n","results_df = pd.concat(results_df,axis = 0)\n","\n","return results_df, train_history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NddXWyNOKJyO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_cX5bEuKKFf"},"source":["def rnn_xsubject_joint_data_train_all_subjects(feature_matrix, target_labels, sub_labels, block_labels, exclude,\\\n","                                                    model_dict, score_list,\n","                                                    verbose = 0, epochs = 40, batch_size = 2, validation_split = 0.1, mv = False, permute = False):\n","    \"\"\"\n","    train and validate a RNN model using data from multiple subjects \n","    train on all subjects\n","    \"\"\"\n","\n","    #subjects in list. there are the units over which we will do train/test split\n","    subs = np.unique(sub_labels)\n","\n","    if permute:\n","        #permute while ignoring excluded blocks\n","        target_labels = permute_class_within_sub(target_labels, block_labels, sub_labels, exclude)\n","\n","\n","    n_scores = len(score_list)\n","    train_subs = subs\n","    train_idxs = np.where(np.isin(sub_labels,train_subs, invert = False))[0]\n","\n","    #get trained model\n","    train_scores, trained_model, scaler, history = get_trained_rnn_model(feature_matrix, target_labels, train_idxs, block_labels, nsets_training,\\\n","                                                                        exclude, model_dict, score_list,\\\n","                                                                        verbose = verbose, epochs = epochs, batch_size = batch_size,\\\n","                                                                        validation_split = validation_split,\\\n","                                                                        mv = mv)\n","\n","\n","    #put in data frame\n","    data_dict = {'Type':'Train'}\n","    for sidx in range(n_scores):\n","        data_dict['%s_score'%(score_list[sidx])] = train_scores[sidx]\n","    results_df = pd.DataFrame(data_dict, index = [0])\n","\n","\n","    return results_df, history, trained_model, scaler"],"execution_count":null,"outputs":[]}]}