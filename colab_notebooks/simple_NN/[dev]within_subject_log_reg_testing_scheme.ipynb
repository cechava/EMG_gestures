{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"within_subject_log_reg_testing_scheme.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP/PjIoB2O8Ds9PYlhNP5td"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TyYD4oUuvm6h","executionInfo":{"status":"ok","timestamp":1627590631957,"user_tz":240,"elapsed":20762,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"2c33f2e5-58a9-4c4c-c925-902db75b50bd"},"source":["#Run cell to mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RVmD2y3kvVQm","executionInfo":{"status":"ok","timestamp":1627590635442,"user_tz":240,"elapsed":3494,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":["#import necessary packages\n","\n","#our workhorses\n","import numpy as np\n","import pandas as pd\n","import scipy\n","\n","#to visualize\n","%matplotlib inline\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","#style params for figures\n","sns.set(font_scale = 2)\n","plt.style.use('seaborn-white')\n","plt.rc(\"axes\", labelweight=\"bold\")\n","from IPython.display import display, HTML\n","\n","#to load files\n","import os\n","import sys\n","import h5py\n","\n","#append repo folder to search path\n","sys.path.append('/content/drive/MyDrive/limb-position-EMG-Repo/')\n","from utils import *\n","\n","from sklearn.model_selection import KFold"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDh_RNK2SztA","executionInfo":{"status":"ok","timestamp":1627590636532,"user_tz":240,"elapsed":750,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":["def get_log_reg_model(input_shape, n_outputs, n_dense_pre = 0, drop_prob = 0.5, activation = 'tanh'):\n","    \"\"\"\n","    Create \n","    \n","    Args:\n","        input_shape\n","        n_outputs: number of output classes\n","        mask_value: value indicating which timepoints to mask out\n","            \n","    Returns:\n","        model\n","    \"\"\"\n","    \n","    #define model architecture\n","    X_input = Input(shape = input_shape)\n","    X = X_input\n","    for n in range(n_dense_pre):\n","        X = Dense(input_shape[1],activation = activation)(X)\n","        X = Dropout(drop_prob)(X)\n","    X = Dense(n_outputs,activation = 'softmax')(X)\n","    model = Model(inputs = X_input, outputs = X)\n","    return model\n","\n","def get_log_reg_f1(X, Y, model, average = 'weighted', mask_value = -100):\n","    \"\"\"\n","    Get f1 score for an RNN model using masked timepoint data\n","\n","    Args:\n","        X: 3D numpy array with shape [samples, timepoints, features]\n","        Y: 3D numpy array with shape [samples, timepoints, classes]. one-hot coding of classes\n","        model: RNN model object\n","        average: string argument for f1_score function. Usually 'macro' or 'weighted'\n","        mask_value: value indicating which timepoints to mask out\n","\n","    Returns:\n","        f1: f1 score\n","    \"\"\"\n","    # Mask out indices based on mask value\n","    nonmasked_idxs = np.where(Y[:,0].flatten()!=mask_value)[0]\n","    # Get target labels for non-masked timepoints\n","    y_true = np.argmax(Y,1).flatten()[nonmasked_idxs]\n","    # Get model predictions for non-masked timepoints\n","    preds = model.predict(X)\n","    y_pred = np.argmax(preds,1).flatten()[nonmasked_idxs]\n","    # Get F1 score\n","    f1 = f1_score(y_true,y_pred,average = average)\n","\n","    return f1\n","\n","def prepare_data_for_log_reg(X,Y, select_idxs, exclude_labels, train = False,scaler = None):\n","\n","    X_cube =  X[select_idxs,:]\n","    Y_cube = Y[select_idxs]\n","\n","    if train:\n","        scaler = StandardScaler()\n","        scaler = scaler.fit(X_cube)\n","        X_cube = scaler.transform(X_cube)\n","    else:\n","        X_cube = scaler.transform(X_cube)\n","\n","    include_idxs = np.where(np.isin(Y_cube,exclude_labels, invert = True))[0]\n","\n","    X_cube = X_cube[include_idxs,:]\n","    Y_cube = Y_cube[include_idxs]\n","    Y_cube = to_categorical(Y_cube-np.min(Y_cube))\n","\n","    return X_cube, Y_cube, scaler\n","def shift_array(arr, num, fill_value=np.nan):\n","    result = np.empty_like(arr)\n","    if num > 0:\n","        result[:num] = fill_value\n","        result[num:] = arr[:-num]\n","    elif num < 0:\n","        result[num:] = fill_value\n","        result[:num] = arr[-num:]\n","    else:\n","        result[:] = arr\n","    return result\n","\n","def get_mv_preds(X, model, n_votes):\n","    #get predictions by majority voting scheme\n","\n","    y_prob = np.squeeze(model.predict(X))\n","    y_pred = np.argmax(y_prob,1)\n","\n","    y_stack = y_pred.astype('float').copy()\n","    y_last = y_pred.astype('float').copy()\n","\n","    for n in range(n_votes):\n","        y_shifted = shift_array(y_last,1)\n","        y_stack = np.vstack((y_stack,y_shifted))\n","        y_last = y_shifted.copy()\n","\n","    y_pred_mv, vote_counts = scipy.stats.mode(y_stack,0,nan_policy='omit')\n","    y_pred_mv = np.squeeze(y_pred_mv.data)\n","\n","    return y_pred_mv\n","\n","def within_subject_log_reg_performance(X, Y, series_labels, exclude,  verbose = 0, epochs = 40, batch_size = 2, mv = False, permute = False):\n","    \n","    #initialize object for k-fold cross-validation\n","    n_splits = np.unique(series_labels).size\n","    kf = KFold(n_splits=n_splits,shuffle = True)\n","    #initialize empty arrays\n","    train_f1_scores = np.empty((n_splits,))\n","    test_f1_scores = np.empty((n_splits,))\n","\n","    for split_count, (series_train, series_test) in enumerate(kf.split(np.unique(series_labels))):\n","        print('Split Count: %i'% (split_count+1))\n","        #get train and test idxs\n","        train_idxs = np.where(series_labels==series_train)[0]\n","        test_idxs = np.where(series_labels==series_test)[0]\n","        #get training data cubes\n","        X_train_cube, Y_train_cube, scaler = prepare_data_for_log_reg(X,Y, train_idxs, exclude, train = True)\n","        if permute:\n","            perm_idxs = np.random.permutation(np.arange(Y_train_cube.shape[0]))\n","            Y_train_cube = Y_train_cube[perm_idxs,:]\n","\n","        n_features, n_outputs = X_train_cube.shape[1], Y_train_cube.shape[1]\n","\n","        #setting timestep dimension to None \n","        model = get_log_reg_model((n_features,),n_outputs)\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        #model.summary\n","\n","        print('Training Model')\n","        # fit network\n","        history = model.fit(X_train_cube, Y_train_cube, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","\n","        # # evaluate trained network\n","        print('Evaluate Model')\n","        \n","\n","        if mv:\n","            # get testing data cubes\n","            X_test_cube, Y_test_cube, scaler = prepare_data_for_log_reg(X,Y, train_idxs, [], train = False, scaler = scaler)\n","            y_pred = get_mv_preds(X_test_cube, model, n_votes= 5)+1\n","            y_true = np.squeeze(np.argmax(Y_test_cube,1))\n","            include_idxs = np.where(np.isin(y_true,exclude, invert = True))[0]\n","            y_true = y_true[include_idxs]\n","            y_pred = y_pred[include_idxs]\n","            train_f1 = f1_score(y_true,y_pred,average = 'weighted')\n","\n","            # get testing data cubes\n","            X_test_cube, Y_test_cube, scaler = prepare_data_for_log_reg(X,Y, test_idxs, [], train = False, scaler = scaler)\n","            y_pred = get_mv_preds(X_test_cube, model, n_votes= 5)+1\n","            y_true = np.squeeze(np.argmax(Y_test_cube,1))\n","            include_idxs = np.where(np.isin(y_true,exclude, invert = True))[0]\n","            y_true = y_true[include_idxs]\n","            y_pred = y_pred[include_idxs]\n","            test_f1 = f1_score(y_true,y_pred,average = 'weighted')\n","        else:\n","            #get score for training data\n","            train_f1 = get_log_reg_f1(X_train_cube, Y_train_cube, model)\n","            # get testing data cubes\n","            X_test_cube, Y_test_cube, scaler = prepare_data_for_log_reg(X,Y, test_idxs, exclude, train = False, scaler = scaler)\n","            #get score for testing data\n","            test_f1 = get_log_reg_f1(X_test_cube, Y_test_cube, model)\n","        #put scores in array\n","        train_f1_scores[split_count] = train_f1\n","        test_f1_scores[split_count] = test_f1\n","\n","    return train_f1_scores, test_f1_scores"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcuPGJLhvv3j","executionInfo":{"status":"ok","timestamp":1627590651067,"user_tz":240,"elapsed":4156,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"f08fe156-7f24-4bd1-a813-17fa9f5253a2"},"source":["#define where the data files are located\n","data_folder = '/content/drive/MyDrive/limb-position-EMG-Repo/EMG_data/'\n","\n","nsubjects = 36\n","\n","\n","# User-defined parameters\n","lo_freq = 20 #lower bound of bandpass filter\n","hi_freq = 450 #upper bound of bandpass filter\n","\n","win_size = 100 #define window size over which to compute time-domain features\n","step = win_size #keeping this parameter in case we want to re-run later with some overlap\n","\n","#excluded labels\n","exclude = [0,7]\n","\n","#for subject_id in range(nsubjects,nsubjects+1):\n","subject_id = 1\n","subject_folder = os.path.join(data_folder,'%02d'%(subject_id))\n","print('=======================')\n","print(subject_folder)\n","\n","# Process data and get features \n","#get features across segments and corresponding info\n","feature_matrix_sub, target_labels_sub, window_tstamps_sub, \\\n","block_labels_sub, series_labels_sub = get_subject_data_for_classification(subject_folder, lo_freq, hi_freq, \\\n","                                                                win_size, step)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["=======================\n","/content/drive/MyDrive/limb-position-EMG-Repo/EMG_data/01\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShJ9Eda4wv5l","executionInfo":{"status":"ok","timestamp":1627591425143,"user_tz":240,"elapsed":754731,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"05b9b9ce-0b19-47c3-b4f3-ab59c4bee5dd"},"source":["nreps = 10\n","exclude = [0,7]#labels to exclude\n","\n","#for RNN training\n","verbose = 0\n","epochs_list = [10,20,40]\n","batch_size_list = [2, 5, 10]\n","# epochs = 40\n","# batch_size = 2\n","\n","results_df = []#initialize empty array for dataframes\n","for epochs in epochs_list:\n","    for batch_size in batch_size_list:\n","        np.random.seed(1)#for reproducibility\n","        for rep in range(nreps):\n","            print('Epochs %d| Batch size %d|Rep %d'%(epochs, batch_size, rep+1))\n","\n","            print('True Data')\n","            train_f1, test_f1 = within_subject_log_reg_performance(feature_matrix_sub, target_labels_sub, series_labels_sub, exclude,\\\n","                                                                                verbose = 0, epochs = epochs, batch_size = batch_size, permute = False)\n","            results_df.append(pd.DataFrame({'F1_score':train_f1,\\\n","                                'Fold':np.arange(train_f1.size)+1,\\\n","                                'Rep':[rep+1 for x in range(train_f1.size)],\\\n","                                'Type':['Train' for x in range(train_f1.size)],\\\n","                                'Shuffled':[False for x in range(train_f1.size)],\\\n","                                'Subject':[subject_id for x in range(train_f1.size)],\\\n","                                'Epochs':[epochs for x in range(train_f1.size)],\\\n","                                'Batch_size':[batch_size for x in range(train_f1.size)],\\\n","                                }))\n","            results_df.append(pd.DataFrame({'F1_score':test_f1,\\\n","                                'Fold':np.arange(test_f1.size)+1,\\\n","                                'Rep':[rep+1 for x in range(test_f1.size)],\\\n","                                'Type':['Test' for x in range(test_f1.size)],\\\n","                                'Shuffled':[False for x in range(test_f1.size)],\\\n","                                'Subject':[subject_id for x in range(test_f1.size)],\\\n","                                'Epochs':[epochs for x in range(test_f1.size)],\\\n","                                'Batch_size':[batch_size for x in range(test_f1.size)],\\\n","                                }))\n","            \n","            print('Permuted Data')\n","            train_f1_perm, test_f1_perm = within_subject_log_reg_performance(feature_matrix_sub, target_labels_sub, series_labels_sub, exclude,\\\n","                                                                                verbose = 0, epochs = epochs, batch_size = batch_size, permute = True)\n","            results_df.append(pd.DataFrame({'F1_score':train_f1_perm,\\\n","                                'Fold':np.arange(train_f1_perm.size)+1,\\\n","                                'Rep':[rep+1 for x in range(train_f1_perm.size)],\\\n","                                'Type':['Train' for x in range(train_f1_perm.size)],\\\n","                                'Shuffled':[True for x in range(train_f1_perm.size)],\\\n","                                'Subject':[subject_id for x in range(train_f1_perm.size)],\\\n","                                'Epochs':[epochs for x in range(train_f1_perm.size)],\\\n","                                'Batch_size':[batch_size for x in range(train_f1_perm.size)],\\\n","                                }))\n","            results_df.append(pd.DataFrame({'F1_score':test_f1_perm,\\\n","                                'Fold':np.arange(test_f1_perm.size)+1,\\\n","                                'Rep':[rep+1 for x in range(test_f1_perm.size)],\\\n","                                'Type':['Test' for x in range(test_f1_perm.size)],\\\n","                                'Shuffled':[True for x in range(test_f1_perm.size)],\\\n","                                'Subject':[subject_id for x in range(test_f1_perm.size)],\\\n","                                'Epochs':[epochs for x in range(test_f1_perm.size)],\\\n","                                'Batch_size':[batch_size for x in range(test_f1_perm.size)],\\\n","                                }))\n","results_df = pd.concat(results_df, axis = 0)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Epochs 10| Batch size 2|Rep 1\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 2|Rep 2\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 2|Rep 3\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 2|Rep 4\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 2|Rep 5\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 2|Rep 6\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 2|Rep 7\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 2|Rep 8\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 2|Rep 9\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 2|Rep 10\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 5|Rep 1\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 5|Rep 2\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 5|Rep 3\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 5|Rep 4\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 5|Rep 5\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 5|Rep 6\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 5|Rep 7\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 5|Rep 8\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 5|Rep 9\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 5|Rep 10\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 10|Rep 1\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 10|Rep 2\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 10|Rep 3\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 10|Rep 4\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 10|Rep 5\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 10|Rep 6\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 10|Rep 7\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 10|Rep 8\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 10|Rep 9\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 10| Batch size 10|Rep 10\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 2|Rep 1\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 2|Rep 2\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 2|Rep 3\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 2|Rep 4\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 2|Rep 5\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 2|Rep 6\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 2|Rep 7\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 2|Rep 8\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 2|Rep 9\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 2|Rep 10\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 5|Rep 1\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 5|Rep 2\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 5|Rep 3\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 5|Rep 4\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 5|Rep 5\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 5|Rep 6\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 5|Rep 7\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 5|Rep 8\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 5|Rep 9\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 5|Rep 10\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 10|Rep 1\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 10|Rep 2\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 10|Rep 3\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 10|Rep 4\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 10|Rep 5\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 10|Rep 6\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 10|Rep 7\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 10|Rep 8\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 10|Rep 9\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 20| Batch size 10|Rep 10\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 2|Rep 1\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 2|Rep 2\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 2|Rep 3\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 2|Rep 4\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 2|Rep 5\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 2|Rep 6\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 2|Rep 7\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 2|Rep 8\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 2|Rep 9\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 2|Rep 10\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 5|Rep 1\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 5|Rep 2\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 5|Rep 3\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 5|Rep 4\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 5|Rep 5\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 5|Rep 6\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 5|Rep 7\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 5|Rep 8\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 5|Rep 9\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 5|Rep 10\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 10|Rep 1\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 10|Rep 2\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 10|Rep 3\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 10|Rep 4\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 10|Rep 5\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 10|Rep 6\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 10|Rep 7\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 10|Rep 8\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 10|Rep 9\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Epochs 40| Batch size 10|Rep 10\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n","Split Count: 1\n","Training Model\n","Evaluate Model\n","Split Count: 2\n","Training Model\n","Evaluate Model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1YCphSTr8ERl","executionInfo":{"status":"ok","timestamp":1627593369304,"user_tz":240,"elapsed":479,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"112586ff-9b1b-42ce-aeba-a85e82afcdc4"},"source":["results_df.groupby(['Type','Shuffled','Epochs','Batch_size']).mean()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>F1_score</th>\n","      <th>Fold</th>\n","      <th>Rep</th>\n","      <th>Subject</th>\n","    </tr>\n","    <tr>\n","      <th>Type</th>\n","      <th>Shuffled</th>\n","      <th>Epochs</th>\n","      <th>Batch_size</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"18\" valign=\"top\">Test</th>\n","      <th rowspan=\"9\" valign=\"top\">False</th>\n","      <th rowspan=\"3\" valign=\"top\">10</th>\n","      <th>2</th>\n","      <td>0.764233</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.634544</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.485749</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">20</th>\n","      <th>2</th>\n","      <td>0.810021</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.776334</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.687989</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">40</th>\n","      <th>2</th>\n","      <td>0.821832</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.815008</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.789799</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"9\" valign=\"top\">True</th>\n","      <th rowspan=\"3\" valign=\"top\">10</th>\n","      <th>2</th>\n","      <td>0.142039</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.145827</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.155334</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">20</th>\n","      <th>2</th>\n","      <td>0.148753</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.148881</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.143433</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">40</th>\n","      <th>2</th>\n","      <td>0.136746</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.126727</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.141767</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"18\" valign=\"top\">Train</th>\n","      <th rowspan=\"9\" valign=\"top\">False</th>\n","      <th rowspan=\"3\" valign=\"top\">10</th>\n","      <th>2</th>\n","      <td>0.834802</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.686842</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.533952</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">20</th>\n","      <th>2</th>\n","      <td>0.898656</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.836102</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.756948</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">40</th>\n","      <th>2</th>\n","      <td>0.920413</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.906281</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.866115</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"9\" valign=\"top\">True</th>\n","      <th rowspan=\"3\" valign=\"top\">10</th>\n","      <th>2</th>\n","      <td>0.209101</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.192797</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.185244</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">20</th>\n","      <th>2</th>\n","      <td>0.232986</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.217763</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.193684</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">40</th>\n","      <th>2</th>\n","      <td>0.267509</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.243967</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.228827</td>\n","      <td>1.5</td>\n","      <td>5.5</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                  F1_score  Fold  Rep  Subject\n","Type  Shuffled Epochs Batch_size                              \n","Test  False    10     2           0.764233   1.5  5.5      1.0\n","                      5           0.634544   1.5  5.5      1.0\n","                      10          0.485749   1.5  5.5      1.0\n","               20     2           0.810021   1.5  5.5      1.0\n","                      5           0.776334   1.5  5.5      1.0\n","                      10          0.687989   1.5  5.5      1.0\n","               40     2           0.821832   1.5  5.5      1.0\n","                      5           0.815008   1.5  5.5      1.0\n","                      10          0.789799   1.5  5.5      1.0\n","      True     10     2           0.142039   1.5  5.5      1.0\n","                      5           0.145827   1.5  5.5      1.0\n","                      10          0.155334   1.5  5.5      1.0\n","               20     2           0.148753   1.5  5.5      1.0\n","                      5           0.148881   1.5  5.5      1.0\n","                      10          0.143433   1.5  5.5      1.0\n","               40     2           0.136746   1.5  5.5      1.0\n","                      5           0.126727   1.5  5.5      1.0\n","                      10          0.141767   1.5  5.5      1.0\n","Train False    10     2           0.834802   1.5  5.5      1.0\n","                      5           0.686842   1.5  5.5      1.0\n","                      10          0.533952   1.5  5.5      1.0\n","               20     2           0.898656   1.5  5.5      1.0\n","                      5           0.836102   1.5  5.5      1.0\n","                      10          0.756948   1.5  5.5      1.0\n","               40     2           0.920413   1.5  5.5      1.0\n","                      5           0.906281   1.5  5.5      1.0\n","                      10          0.866115   1.5  5.5      1.0\n","      True     10     2           0.209101   1.5  5.5      1.0\n","                      5           0.192797   1.5  5.5      1.0\n","                      10          0.185244   1.5  5.5      1.0\n","               20     2           0.232986   1.5  5.5      1.0\n","                      5           0.217763   1.5  5.5      1.0\n","                      10          0.193684   1.5  5.5      1.0\n","               40     2           0.267509   1.5  5.5      1.0\n","                      5           0.243967   1.5  5.5      1.0\n","                      10          0.228827   1.5  5.5      1.0"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"0NMFugtA9ILG","executionInfo":{"status":"ok","timestamp":1627593421394,"user_tz":240,"elapsed":1252,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":["results_folder = '/content/drive/MyDrive/limb-position-EMG-Repo/results_data/single_subject_training/log_reg/'\n","#save results to file\n","results_fn = 'subject_%02d_training_scheme_results.h5'%(subject_id)\n","results_df.to_hdf(os.path.join(results_folder,results_fn), key='results_df', mode='w')\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"2X4UVQWg0NGy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3lgq69ES0QeR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qhQCzXmAwKVH","executionInfo":{"status":"ok","timestamp":1627590205801,"user_tz":240,"elapsed":144,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}}},"source":[""],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6HaENmAaRt3d","executionInfo":{"status":"ok","timestamp":1627582791134,"user_tz":240,"elapsed":157,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"cbd83a6f-f416-48b8-89d9-81999ea1c88a"},"source":["n_splits = np.unique(series_labels).size\n","X = feature_matrix_sub.copy()\n","Y = target_labels_sub.copy()\n","exclude =[0,7]\n","\n","#for RNN training\n","verbose = 0\n","epochs = 40\n","batch_size = 2\n","\n","\n","\n","\n","\n","\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Split Count: 1\n","Split Count: 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n75-9CNnhE0d"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FRlWbgFdhhGy","executionInfo":{"status":"ok","timestamp":1627583441796,"user_tz":240,"elapsed":5915,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"35124c2a-8976-498f-adf8-b939924b89dc"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oconnsechmG-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCrXLsr0hub9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-IhMG9wjxqw","executionInfo":{"status":"ok","timestamp":1627583908321,"user_tz":240,"elapsed":13,"user":{"displayName":"Cesar Echavarria","photoUrl":"","userId":"12535184235153277849"}},"outputId":"ebf333b5-7259-4446-b695-96b94fd4cf15"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(593, 7)"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"26-AW_Ulh3C-"},"source":[""],"execution_count":null,"outputs":[]}]}