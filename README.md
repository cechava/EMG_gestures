# limb-position-EMG

Personal project exploring different types of classifiers for the task of classifying upper-limb gestures from EMG data collected with a MYO thalmic bracelet. This repository contains notebooks with different stages of development of a robust classification algorithm.

The following jupyter notebooks provide an overview of current project progress and other details:
+ single_subject_classfication_demo.ipynb: walk-through of technical approach and data. Analysis results for a single subject using two different types of classifier models.
+ compare_model_performance_within_and_across_subjects.ipynb: comparing ability of different types of classifier models to generalize callibration across subjects

The data set was provided by the UC Irvine ML Repository and can be downloaded from: http://archive.ics.uci.edu/ml/datasets/EMG+data+for+gestures#

**Currently working on extending model for robustness across subjects**

